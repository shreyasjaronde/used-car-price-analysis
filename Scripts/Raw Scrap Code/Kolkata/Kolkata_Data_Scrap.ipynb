{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b96f469-6218-4a24-ba3d-912d3cadca82",
   "metadata": {},
   "source": [
    "1st stage off scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55cbd8a9-053a-4a53-9260-75b1ae5cb812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected total_listings=1365, per_page_guess=68, total_pages=21\n",
      "Scanned page 10; collected rows so far: 871\n",
      "Scanned page 20; collected rows so far: 880\n",
      "Collected 884 card-level rows, discovered 850 detail links.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\866317437.py:316: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\866317437.py:332: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 detail pages...\n",
      "Processed 200 detail pages...\n",
      "Processed 300 detail pages...\n",
      "Processed 400 detail pages...\n",
      "Processed 500 detail pages...\n",
      "Processed 600 detail pages...\n",
      "Processed 700 detail pages...\n",
      "Processed 800 detail pages...\n",
      "Done. Collected 851 rows. Saved to cardekho_used_cars_kolkata_with_mileage.csv and cardekho_used_cars_kolkata_with_mileage.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cardekho_used_kolkata_scraper_with_mileage.py\n",
    "\n",
    "Scrapes https://www.cardekho.com/used-cars+in+kolkata and writes CSV + Excel.\n",
    "Extracted columns:\n",
    "Car_name, brand, model, kms_driven, mileage, transmission, fuel_type, year_of_manufacture, price, detail_page\n",
    "\n",
    "Requires: selenium, beautifulsoup4, pandas, openpyxl, webdriver-manager\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------- CONFIG ----------\n",
    "# START_URL = \"https://www.cardekho.com/used-cars+in+hyderabad\"\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+kolkata\"   # URL for kolkata\n",
    "OUTPUT_CSV = \"cardekho_used_cars_kolkata_with_mileage.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_kolkata_with_mileage.xlsx\"\n",
    "\n",
    "HEADLESS = True                 # Set False to watch browser\n",
    "MAX_PAGES_OVERRIDE = None       # Set to int to force how many result pages to scan; None -> auto-detect\n",
    "MAX_SCROLLS = 40                # scroll rounds per search-result page\n",
    "SCROLL_PAUSE = 0.7\n",
    "PAGE_PAUSE = (0.6, 1.4)\n",
    "VISIT_DETAIL_PAGES = True       # True -> open each listing detail page for mileage/transmission accuracy\n",
    "DETAIL_PAUSE = (0.6, 1.2)\n",
    "MAX_DETAIL_RETRIES = 1         # retry detail page once on failure\n",
    "# Brands to help split brand/model\n",
    "BRANDS = [\n",
    "    \"Maruti\", \"Hyundai\", \"Tata\", \"Honda\", \"Toyota\", \"Mahindra\", \"Kia\",\n",
    "    \"BMW\", \"Audi\", \"Mercedes-Benz\", \"Mercedes\", \"Renault\", \"MG\", \"Skoda\",\n",
    "    \"Volkswagen\", \"Ford\", \"Nissan\", \"Jeep\", \"Volvo\", \"Land Rover\", \"Jaguar\",\n",
    "    \"Isuzu\", \"Datsun\", \"Chevrolet\", \"Opel\"\n",
    "]\n",
    "# --------------------------\n",
    "\n",
    "# Helper parsers\n",
    "def guess_brand_and_model(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in name.lower():\n",
    "            model = re.sub(re.escape(b), \"\", name, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+', '', model)\n",
    "            return b, model or name\n",
    "    parts = name.split()\n",
    "    return (parts[0], \" \".join(parts[1:]) if len(parts) > 1 else \"\") if parts else (\"\",\"\")\n",
    "\n",
    "def extract_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_fuel(text):\n",
    "    for f in [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\", \"Electric\", \"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "def extract_price(text):\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    m2 = re.search(r'[\\d\\.,]+\\s*(Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(0)\n",
    "    return \"\"\n",
    "\n",
    "def extract_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# mileage pattern examples:\n",
    "# 18.5 kmpl, 22 km/kg, 120 km/kWh, 18 kmpl (ARAI), 25.6 kmpl\n",
    "MILEAGE_REGEXES = [\n",
    "    re.compile(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|km/kg|km/kwh|km/l|kmperlitre|km/gal|km/100km|kml|kpl)\\b', flags=re.I),\n",
    "    re.compile(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(km(?:/kwh|/kg|pl)?)\\b', flags=re.I),\n",
    "    re.compile(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(mpg|mpg\\;)', flags=re.I)\n",
    "]\n",
    "\n",
    "def extract_mileage(text):\n",
    "    txt = text.replace(\"\\xa0\",\" \").strip()\n",
    "    for rx in MILEAGE_REGEXES:\n",
    "        m = rx.search(txt)\n",
    "        if m:\n",
    "            val = m.group(1)\n",
    "            unit = m.group(2)\n",
    "            return f\"{val} {unit}\".strip()\n",
    "    # sometimes written like \"Mileage: 18.5 kmpl\" or \"18.5kmpl\"\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|km/kg|km/kwh|km|kpl|km/l)\\b', txt, flags=re.I)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} {m2.group(2)}\"\n",
    "    return \"\"\n",
    "\n",
    "# transmission extraction\n",
    "def extract_transmission(text):\n",
    "    for t in [\"Manual\", \"Automatic\", \"CVT\", \"AMT\", \"DCT\", \"AT\", \"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            # normalize common variants\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "# safe text extractor for BeautifulSoup element\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    # Setup Selenium Chrome\n",
    "    chrome_opts = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_opts.add_argument(\"--disable-gpu\")\n",
    "    chrome_opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "\n",
    "    try:\n",
    "        # load first page and detect pages count\n",
    "        driver.get(START_URL)\n",
    "        time.sleep(2.0)\n",
    "        soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        page_text = soup0.get_text(\" \", strip=True)\n",
    "\n",
    "        total_listings = None\n",
    "        m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Kolkata', page_text, flags=re.I)\n",
    "        if not m:\n",
    "            m = re.search(r'of\\s+([\\d,]+)\\s+results', page_text, flags=re.I)\n",
    "        if m:\n",
    "            total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "        per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "        estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "        total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "        if MAX_PAGES_OVERRIDE:\n",
    "            total_pages = MAX_PAGES_OVERRIDE\n",
    "\n",
    "        print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages={total_pages}\")\n",
    "\n",
    "        rows = []\n",
    "        seen_keys = set()\n",
    "        detail_links = []\n",
    "\n",
    "        # iterate search result pages\n",
    "        for p in range(1, total_pages + 1):\n",
    "            page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "            except Exception:\n",
    "                time.sleep(1.0)\n",
    "                driver.get(page_url)\n",
    "\n",
    "            # aggressively scroll to let lazy content load\n",
    "            last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            scroll_count = 0\n",
    "            while scroll_count < MAX_SCROLLS:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(SCROLL_PAUSE)\n",
    "                new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_h == last_h:\n",
    "                    # wiggle\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                    time.sleep(0.4)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0.4)\n",
    "                    new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_h == last_h:\n",
    "                        break\n",
    "                last_h = new_h\n",
    "                scroll_count += 1\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # find title nodes (h3) and extract card container text\n",
    "            titles = page_soup.find_all(\"h3\")\n",
    "            for h in titles:\n",
    "                title = text_of(h)\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                # climb up to find a container with price or kms present\n",
    "                container = h\n",
    "                card_text = \"\"\n",
    "                link = \"\"\n",
    "                for _ in range(6):\n",
    "                    if container is None:\n",
    "                        break\n",
    "                    card_text = text_of(container)\n",
    "                    if \"₹\" in card_text or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I):\n",
    "                        # find anchor inside container for detail link if present\n",
    "                        a = container.find(\"a\", href=True)\n",
    "                        if a:\n",
    "                            href = a[\"href\"]\n",
    "                            link = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                            link = link.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                        break\n",
    "                    container = container.parent\n",
    "\n",
    "                if not card_text:\n",
    "                    # fallback to grabbing parent text\n",
    "                    parent = h.parent\n",
    "                    card_text = text_of(parent) if parent else title\n",
    "\n",
    "                # check minimal heuristics: price or kms exist\n",
    "                if (\"₹\" not in card_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I) is None):\n",
    "                    continue\n",
    "\n",
    "                # dedupe by title+price\n",
    "                price_snip = extract_price(card_text)\n",
    "                key = (title + \"||\" + price_snip).strip()\n",
    "                if key in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(key)\n",
    "\n",
    "                kms = extract_kms(card_text)\n",
    "                fuel = extract_fuel(card_text)\n",
    "                year = extract_year(title) or extract_year(card_text)\n",
    "                brand, model = guess_brand_and_model(title)\n",
    "                price = price_snip\n",
    "\n",
    "                # prepare base row (mileage/transmission may be blank now; fill from detail page if VISIT_DETAIL_PAGES)\n",
    "                rows.append({\n",
    "                    \"Car_name\": title,\n",
    "                    \"brand\": brand,\n",
    "                    \"model\": model,\n",
    "                    \"kms_driven\": kms,\n",
    "                    \"mileage\": \"\",             # to fill\n",
    "                    \"transmission\": \"\",       # to fill\n",
    "                    \"fuel_type\": fuel,\n",
    "                    \"year_of_manufacture\": year,\n",
    "                    \"price\": price,\n",
    "                    \"detail_page\": link\n",
    "                })\n",
    "\n",
    "                if link:\n",
    "                    detail_links.append(link)\n",
    "\n",
    "            # jitter between pages\n",
    "            time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "            # early stop if found >= total_listings\n",
    "            if total_listings and len(seen_keys) >= total_listings:\n",
    "                print(\"Reached detected total listings; stopping page scan.\")\n",
    "                break\n",
    "\n",
    "            # small progress print\n",
    "            if p % 10 == 0:\n",
    "                print(f\"Scanned page {p}; collected rows so far: {len(rows)}\")\n",
    "\n",
    "        print(f\"Collected {len(rows)} card-level rows, discovered {len(set(detail_links))} detail links.\")\n",
    "\n",
    "        # Optionally visit detail pages for mileage + transmission (more accurate)\n",
    "        if VISIT_DETAIL_PAGES and detail_links:\n",
    "            unique_detail_links = []\n",
    "            seen_dl = set()\n",
    "            for u in detail_links:\n",
    "                if u and u not in seen_dl:\n",
    "                    seen_dl.add(u)\n",
    "                    unique_detail_links.append(u)\n",
    "\n",
    "            # map detail_link -> parsed fields\n",
    "            detail_map = {}\n",
    "\n",
    "            for i, dl in enumerate(unique_detail_links):\n",
    "                # small polite wait\n",
    "                if i > 0:\n",
    "                    time.sleep(random.uniform(*DETAIL_PAUSE))\n",
    "\n",
    "                # retry loop\n",
    "                attempt = 0\n",
    "                success = False\n",
    "                while attempt <= MAX_DETAIL_RETRIES and not success:\n",
    "                    try:\n",
    "                        driver.get(dl)\n",
    "                        time.sleep(1.0 + random.random()*0.8)  # allow JS\n",
    "                        dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        page_text = dsoup.get_text(\" \", strip=True)\n",
    "\n",
    "                        # Try structured lookups:\n",
    "                        # 1) meta-line near the H1/H2 (often contains \"kms • Petrol • 2019 • 18.5 kmpl • Automatic\")\n",
    "                        h_tag = dsoup.find([\"h1\",\"h2\"])\n",
    "                        meta_line = \"\"\n",
    "                        if h_tag:\n",
    "                            nxt = h_tag.find_next()\n",
    "                            checks = 0\n",
    "                            while nxt and checks < 8:\n",
    "                                t = text_of(nxt)\n",
    "                                if t and ((\"kms\" in t.lower()) or (\"₹\" in t) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', t, flags=re.I)):\n",
    "                                    meta_line = t\n",
    "                                    break\n",
    "                                nxt = nxt.find_next()\n",
    "                                checks += 1\n",
    "\n",
    "                        # 2) labeled field lookup: look for text nodes like 'Mileage' or 'Transmission' and read siblings\n",
    "                        # Mileage\n",
    "                        mileage_val = \"\"\n",
    "                        # find text nodes \"Mileage\" or \"Avg. Mileage\" etc.\n",
    "                        mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                        if mnode:\n",
    "                            try:\n",
    "                                # sibling / next element may contain value\n",
    "                                parent = mnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    mileage_val = text_of(sib)\n",
    "                            except:\n",
    "                                mileage_val = \"\"\n",
    "                        if not mileage_val:\n",
    "                            # try meta_line and page_text\n",
    "                            mileage_val = extract_mileage(meta_line or page_text) or extract_mileage(page_text)\n",
    "\n",
    "                        # Transmission\n",
    "                        trans_val = \"\"\n",
    "                        tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n",
    "                        if tnode:\n",
    "                            try:\n",
    "                                parent = tnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    trans_val = text_of(sib)\n",
    "                            except:\n",
    "                                trans_val = \"\"\n",
    "                        if not trans_val:\n",
    "                            trans_val = extract_transmission(meta_line or page_text)\n",
    "\n",
    "                        # Normalize/massage values\n",
    "                        mileage_clean = extract_mileage(mileage_val or \"\")\n",
    "                        transmission_clean = extract_transmission(trans_val or \"\")\n",
    "\n",
    "                        # fallback: sometimes card-level has mileage like '18 kmpl' in small text — try to extract from page text\n",
    "                        if not mileage_clean:\n",
    "                            mileage_clean = extract_mileage(page_text)\n",
    "\n",
    "                        # store\n",
    "                        detail_map[dl] = {\n",
    "                            \"mileage\": mileage_clean,\n",
    "                            \"transmission\": transmission_clean\n",
    "                        }\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt > MAX_DETAIL_RETRIES:\n",
    "                            # give empty values on failure\n",
    "                            detail_map[dl] = {\"mileage\": \"\", \"transmission\": \"\"}\n",
    "                            success = True\n",
    "                        else:\n",
    "                            time.sleep(0.8)\n",
    "\n",
    "                # progress print occasionally\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} detail pages...\")\n",
    "\n",
    "            # Now merge detail_map into rows\n",
    "            for r in rows:\n",
    "                link = r.get(\"detail_page\", \"\")\n",
    "                if link and link in detail_map:\n",
    "                    r[\"mileage\"] = detail_map[link][\"mileage\"]\n",
    "                    r[\"transmission\"] = detail_map[link][\"transmission\"]\n",
    "                else:\n",
    "                    # attempt to find mileage/trans in the card text (already tried earlier)\n",
    "                    # leave empty if not found\n",
    "                    if not r[\"mileage\"]:\n",
    "                        # try to infer from model/name\n",
    "                        r[\"mileage\"] = \"\"\n",
    "                    if not r[\"transmission\"]:\n",
    "                        r[\"transmission\"] = \"\"\n",
    "\n",
    "        # build DataFrame and clean\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            \"Car_name\", \"brand\", \"model\", \"kms_driven\", \"mileage\", \"transmission\",\n",
    "            \"fuel_type\", \"year_of_manufacture\", \"price\", \"detail_page\"\n",
    "        ])\n",
    "\n",
    "        # normalize\n",
    "        df[\"kms_driven\"] = df[\"kms_driven\"].fillna(\"\").astype(str).apply(lambda x: re.sub(r'[^\\d\\.]', '', x))\n",
    "        df[\"mileage\"] = df[\"mileage\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"transmission\"] = df[\"transmission\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "        df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].fillna(\"\").astype(str).apply(lambda x: (re.search(r'\\b(19|20)\\d{2}\\b', x).group(0) if re.search(r'\\b(19|20)\\d{2}\\b', x) else \"\"))\n",
    "        df[\"price\"] = df[\"price\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"fuel_type\"] = df[\"fuel_type\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "\n",
    "        # dedupe by detail_link if present else by Car_name+price\n",
    "        if df[\"detail_page\"].notnull().sum() > 0:\n",
    "            df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=[\"Car_name\", \"price\"]).reset_index(drop=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"Done. Collected {len(df)} rows. Saved to {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca2ad0-383e-4036-bb95-784bea6dd7ca",
   "metadata": {},
   "source": [
    "2nd stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbeb0f71-9d62-4f44-9e32-37241aef4a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected total_listings=1365, per_page_guess=68, total_pages=21\n",
      "Scanned page 10; rows so far: 874\n",
      "Scanned page 20; rows so far: 878\n",
      "Collected 878 card-level rows, detail links: 860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\3578145748.py:339: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\3578145748.py:354: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 detail pages...\n",
      "Processed 200 detail pages...\n",
      "Processed 300 detail pages...\n",
      "Processed 400 detail pages...\n",
      "Processed 500 detail pages...\n",
      "Processed 600 detail pages...\n",
      "Processed 700 detail pages...\n",
      "Processed 800 detail pages...\n",
      "Saved 861 rows to cardekho_used_cars_kolkata_price_fixed.csv and cardekho_used_cars_kolkata_price_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cardekho_with_price_fix.py\n",
    "\n",
    "Improved scraper for CarDekho used cars (Kolkata) with robust price extraction.\n",
    "Requires: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------- CONFIG ----------\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+kolkata\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_kolkata_price_fixed.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_kolkata_price_fixed.xlsx\"\n",
    "\n",
    "HEADLESS = True\n",
    "MAX_PAGES_OVERRIDE = None\n",
    "MAX_SCROLLS = 40\n",
    "SCROLL_PAUSE = 0.7\n",
    "PAGE_PAUSE = (0.6, 1.4)\n",
    "VISIT_DETAIL_PAGES = True\n",
    "DETAIL_PAUSE = (0.6, 1.2)\n",
    "MAX_DETAIL_RETRIES = 1\n",
    "\n",
    "BRANDS = [\n",
    "    \"Maruti\", \"Hyundai\", \"Tata\", \"Honda\", \"Toyota\", \"Mahindra\", \"Kia\",\n",
    "    \"BMW\", \"Audi\", \"Mercedes-Benz\", \"Mercedes\", \"Renault\", \"MG\", \"Skoda\",\n",
    "    \"Volkswagen\", \"Ford\", \"Nissan\", \"Jeep\", \"Volvo\", \"Land Rover\", \"Jaguar\",\n",
    "    \"Isuzu\", \"Datsun\", \"Chevrolet\", \"Opel\"\n",
    "]\n",
    "# --------------------------\n",
    "\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def guess_brand_and_model(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in name.lower():\n",
    "            model = re.sub(re.escape(b), \"\", name, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+', '', model)\n",
    "            return b, model or name\n",
    "    parts = name.split()\n",
    "    return (parts[0], \" \".join(parts[1:]) if len(parts) > 1 else \"\") if parts else (\"\",\"\")\n",
    "\n",
    "def extract_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_fuel(text):\n",
    "    for f in [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\", \"Electric\", \"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "# NEW: robust price extraction from soup and raw text\n",
    "def extract_price_from_soup(soup):\n",
    "    # 1. meta tags\n",
    "    meta_selectors = [\n",
    "        ('meta', {'property': 'og:price:amount'}),\n",
    "        ('meta', {'itemprop': 'price'}),\n",
    "        ('meta', {'name': 'price'}),\n",
    "    ]\n",
    "    for tag, attrs in meta_selectors:\n",
    "        mtag = soup.find(tag, attrs=attrs)\n",
    "        if mtag:\n",
    "            val = mtag.get('content') or mtag.get('value') or \"\"\n",
    "            if val:\n",
    "                # normalize: prepend ₹ if numeric and no symbol\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 2. attributes that often store price\n",
    "    for attr in ('data-price', 'data-offer-price', 'data-srp', 'data-amount', 'data-price-value'):\n",
    "        el = soup.find(attrs={attr: True})\n",
    "        if el:\n",
    "            val = el.get(attr)\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 3. elements with class or id containing 'price' or 'amount'\n",
    "    px = soup.find(lambda tag: tag.name in (\"div\",\"span\",\"p\",\"strong\") and (\n",
    "        tag.get(\"class\") or tag.get(\"id\")\n",
    "    ) and re.search(r'price|amount|selling|srp|finalPrice|carPrice|actual-price', \" \".join((tag.get(\"class\") or []) + [tag.get(\"id\") or \"\"]), flags=re.I))\n",
    "    if px:\n",
    "        txt = text_of(px)\n",
    "        pr = find_rupee_in_text(txt)\n",
    "        if pr:\n",
    "            return pr\n",
    "        if txt:\n",
    "            return txt.strip()\n",
    "\n",
    "    # 4. any visible text near top with rupee sign\n",
    "    top_region = \"\"\n",
    "    # try header / top sections\n",
    "    head_candidates = soup.find_all([\"header\", \"section\", \"div\"], limit=6)\n",
    "    for c in head_candidates:\n",
    "        t = text_of(c)\n",
    "        if '₹' in t:\n",
    "            top_region = t\n",
    "            break\n",
    "    if not top_region:\n",
    "        # fallback full page text (takes last resort)\n",
    "        top_region = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    pr = find_rupee_in_text(top_region)\n",
    "    if pr:\n",
    "        return pr\n",
    "\n",
    "    # 5. fallback regex on whole page (Lakh/Crore)\n",
    "    p2 = re.search(r'[\\d\\.,]+\\s*(?:Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', soup.get_text(\" \", strip=True))\n",
    "    if p2:\n",
    "        return p2.group(0).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def find_rupee_in_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    # sometimes rupee symbol is missing but values use lakh/crore\n",
    "    m2 = re.search(r'[\\d\\.,]+\\s*(Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    # ensures same fallback when only raw text available\n",
    "    p = find_rupee_in_text(text)\n",
    "    return p\n",
    "\n",
    "def extract_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# main\n",
    "def main():\n",
    "    chrome_opts = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "\n",
    "    try:\n",
    "        driver.get(START_URL)\n",
    "        time.sleep(2.0)\n",
    "        soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        text0 = soup0.get_text(\" \", strip=True)\n",
    "\n",
    "        total_listings = None\n",
    "        m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Kolkata', text0, flags=re.I)\n",
    "        if not m:\n",
    "            m = re.search(r'of\\s+([\\d,]+)\\s+results', text0, flags=re.I)\n",
    "        if m:\n",
    "            total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "        per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "        estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "        total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "        if MAX_PAGES_OVERRIDE:\n",
    "            total_pages = MAX_PAGES_OVERRIDE\n",
    "\n",
    "        print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages={total_pages}\")\n",
    "\n",
    "        rows = []\n",
    "        seen_keys = set()\n",
    "        detail_links = []\n",
    "\n",
    "        for p in range(1, total_pages + 1):\n",
    "            page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "            except Exception:\n",
    "                time.sleep(1.0)\n",
    "                driver.get(page_url)\n",
    "\n",
    "            # scroll aggressively\n",
    "            last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            sc = 0\n",
    "            while sc < MAX_SCROLLS:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(SCROLL_PAUSE)\n",
    "                new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_h == last_h:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                    time.sleep(0.4)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0.4)\n",
    "                    new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_h == last_h:\n",
    "                        break\n",
    "                last_h = new_h\n",
    "                sc += 1\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            titles = page_soup.find_all(\"h3\")\n",
    "            for h in titles:\n",
    "                title = text_of(h)\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                container = h\n",
    "                card_text = \"\"\n",
    "                link = \"\"\n",
    "                for _ in range(6):\n",
    "                    if container is None:\n",
    "                        break\n",
    "                    card_text = text_of(container)\n",
    "                    if \"₹\" in card_text or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I):\n",
    "                        a = container.find(\"a\", href=True)\n",
    "                        if a:\n",
    "                            href = a[\"href\"]\n",
    "                            link = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                            link = link.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                        break\n",
    "                    container = container.parent\n",
    "\n",
    "                if not card_text:\n",
    "                    parent = h.parent\n",
    "                    card_text = text_of(parent) if parent else title\n",
    "\n",
    "                if (\"₹\" not in card_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I) is None):\n",
    "                    continue\n",
    "\n",
    "                price_card = extract_price_from_text(card_text)\n",
    "                key = (title + \"||\" + (price_card or \"\")).strip()\n",
    "                if key in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(key)\n",
    "\n",
    "                kms = extract_kms(card_text)\n",
    "                fuel = extract_fuel(card_text)\n",
    "                year = extract_year(title) or extract_year(card_text)\n",
    "                brand = \"\"\n",
    "                model = \"\"\n",
    "                try:\n",
    "                    brand, model = guess_brand_and_model(title)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                rows.append({\n",
    "                    \"Car_name\": title,\n",
    "                    \"brand\": brand,\n",
    "                    \"model\": model,\n",
    "                    \"kms_driven\": kms,\n",
    "                    \"mileage\": \"\",\n",
    "                    \"transmission\": \"\",\n",
    "                    \"fuel_type\": fuel,\n",
    "                    \"year_of_manufacture\": year,\n",
    "                    \"price\": price_card,\n",
    "                    \"detail_page\": link\n",
    "                })\n",
    "\n",
    "                if link:\n",
    "                    detail_links.append(link)\n",
    "\n",
    "            time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "            if total_listings and len(seen_keys) >= total_listings:\n",
    "                print(\"Reached detected total listings; stopping page scan.\")\n",
    "                break\n",
    "\n",
    "            if p % 10 == 0:\n",
    "                print(f\"Scanned page {p}; rows so far: {len(rows)}\")\n",
    "\n",
    "        print(f\"Collected {len(rows)} card-level rows, detail links: {len(set(detail_links))}\")\n",
    "\n",
    "        # Now ensure price is filled: visit detail pages for any row missing price\n",
    "        if VISIT_DETAIL_PAGES and detail_links:\n",
    "            unique_detail_links = []\n",
    "            seen_dl = set()\n",
    "            for u in detail_links:\n",
    "                if u and u not in seen_dl:\n",
    "                    seen_dl.add(u)\n",
    "                    unique_detail_links.append(u)\n",
    "\n",
    "            # map link -> price (and optionally mileage/transmission)\n",
    "            detail_map = {}\n",
    "\n",
    "            for i, dl in enumerate(unique_detail_links):\n",
    "                # polite pause\n",
    "                if i > 0:\n",
    "                    time.sleep(random.uniform(*DETAIL_PAUSE))\n",
    "                attempt = 0\n",
    "                success = False\n",
    "                while attempt <= MAX_DETAIL_RETRIES and not success:\n",
    "                    try:\n",
    "                        driver.get(dl)\n",
    "                        time.sleep(1.0 + random.random()*0.8)\n",
    "                        dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                        # price from many possible places\n",
    "                        price_val = extract_price_from_soup(dsoup)\n",
    "                        # fallback to regex on page text\n",
    "                        if not price_val:\n",
    "                            price_val = extract_price_from_text(dsoup.get_text(\" \", strip=True))\n",
    "\n",
    "                        # mileage and transmission extraction as before\n",
    "                        page_text = dsoup.get_text(\" \", strip=True)\n",
    "                        # try meta-line near title for quick values\n",
    "                        h_tag = dsoup.find([\"h1\",\"h2\"])\n",
    "                        meta_line = \"\"\n",
    "                        if h_tag:\n",
    "                            nxt = h_tag.find_next()\n",
    "                            checks = 0\n",
    "                            while nxt and checks < 8:\n",
    "                                t = text_of(nxt)\n",
    "                                if t and ((\"kms\" in t.lower()) or (\"₹\" in t) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', t, flags=re.I)):\n",
    "                                    meta_line = t\n",
    "                                    break\n",
    "                                nxt = nxt.find_next()\n",
    "                                checks += 1\n",
    "\n",
    "                        # mileage\n",
    "                        mileage_val = \"\"\n",
    "                        mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                        if mnode:\n",
    "                            try:\n",
    "                                parent = mnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    mileage_val = text_of(sib)\n",
    "                            except:\n",
    "                                mileage_val = \"\"\n",
    "                        if not mileage_val:\n",
    "                            # check meta_line then page_text\n",
    "                            mileage_val = extract_mileage(meta_line or page_text) or extract_mileage(page_text)\n",
    "\n",
    "                        # transmission\n",
    "                        trans_val = \"\"\n",
    "                        tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n",
    "                        if tnode:\n",
    "                            try:\n",
    "                                parent = tnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    trans_val = text_of(sib)\n",
    "                            except:\n",
    "                                trans_val = \"\"\n",
    "                        if not trans_val:\n",
    "                            trans_val = extract_transmission(meta_line or page_text)\n",
    "\n",
    "                        detail_map[dl] = {\n",
    "                            \"price\": price_val or \"\",\n",
    "                            \"mileage\": mileage_val or \"\",\n",
    "                            \"transmission\": trans_val or \"\"\n",
    "                        }\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt > MAX_DETAIL_RETRIES:\n",
    "                            detail_map[dl] = {\"price\": \"\", \"mileage\": \"\", \"transmission\": \"\"}\n",
    "                            success = True\n",
    "                        else:\n",
    "                            time.sleep(0.8)\n",
    "\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} detail pages...\")\n",
    "\n",
    "            # merge into rows\n",
    "            for r in rows:\n",
    "                link = r.get(\"detail_page\", \"\")\n",
    "                if link and link in detail_map:\n",
    "                    # prefer detail price if card-level empty\n",
    "                    if not r.get(\"price\"):\n",
    "                        r[\"price\"] = detail_map[link][\"price\"]\n",
    "                    # always try to fill mileage/trans if available\n",
    "                    if not r.get(\"mileage\"):\n",
    "                        r[\"mileage\"] = detail_map[link][\"mileage\"]\n",
    "                    if not r.get(\"transmission\"):\n",
    "                        r[\"transmission\"] = detail_map[link][\"transmission\"]\n",
    "\n",
    "        # final normalization\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            \"Car_name\",\"brand\",\"model\",\"kms_driven\",\"mileage\",\"transmission\",\"fuel_type\",\"year_of_manufacture\",\"price\",\"detail_page\"\n",
    "        ])\n",
    "        df[\"kms_driven\"] = df[\"kms_driven\"].fillna(\"\").astype(str).apply(lambda x: re.sub(r'[^\\d\\.]', '', x))\n",
    "        df[\"mileage\"] = df[\"mileage\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"transmission\"] = df[\"transmission\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "        df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].fillna(\"\").astype(str).apply(lambda x: (re.search(r'\\b(19|20)\\d{2}\\b', x).group(0) if re.search(r'\\b(19|20)\\d{2}\\b', x) else \"\"))\n",
    "        df[\"price\"] = df[\"price\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"fuel_type\"] = df[\"fuel_type\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "\n",
    "        # dedupe\n",
    "        if \"detail_page\" in df.columns and df[\"detail_page\"].str.len().sum() > 0:\n",
    "            df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=[\"Car_name\",\"price\"]).reset_index(drop=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# helpers used in detail extraction (mileage/transmission)\n",
    "def extract_mileage(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # reuse a robust regex set (common patterns)\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|km/kg|km/kwh|km/l|kpl|km)', text, flags=re.I)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2)}\".strip()\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(mpg)\\b', text, flags=re.I)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} {m2.group(2)}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ebfce-06fc-41e9-9d35-f352593b259b",
   "metadata": {},
   "source": [
    "3rd Stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14314048-18cb-44e3-89b8-73c7f63421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell: Clean Selenium scraper (improved from your first working script) + mileage extraction\n",
    "# Produces: Car_name, brand, model, kms_driven, fuel_type, year_of_manufacture, price, mileage, detail_page\n",
    "# Saves to cardekho_used_cars_pune_clean_fixed.csv and .xlsx\n",
    "# Requirements: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\n",
    "import re, time, random, math\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------- CONFIG --------------\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+kolkata\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_kolkata_clean_fixed.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_kolkata_clean_fixed.xlsx\"\n",
    "HEADLESS = True               # set False to see browser for debugging\n",
    "MAX_PAGES_OVERRIDE = None     # set an int to force more pages, else auto-detect\n",
    "MAX_SCROLL_ROUNDS = 60        # number of scroll attempts per page (increase to load more)\n",
    "SCROLL_PAUSE = 0.6            # seconds between scrolls\n",
    "PAGE_PAUSE = (0.8, 1.6)       # jitter after loading a page\n",
    "# Basic brand list to split brand/model (optional)\n",
    "BRANDS = [\"Maruti\",\"Hyundai\",\"Tata\",\"Honda\",\"Toyota\",\"Mahindra\",\"Kia\",\"BMW\",\"Audi\",\"Mercedes-Benz\",\n",
    "          \"Renault\",\"MG\",\"Skoda\",\"Volkswagen\",\"Ford\",\"Nissan\",\"Jeep\",\"Volvo\",\"Land Rover\",\"Jaguar\",\n",
    "          \"Isuzu\",\"Datsun\",\"Chevrolet\",\"Opel\"]\n",
    "# -------------------------------------\n",
    "\n",
    "# helpers\n",
    "def guess_brand_and_model(title):\n",
    "    if not title: return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in title.lower():\n",
    "            brand = b\n",
    "            model = re.sub(re.escape(b), \"\", title, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+','', model)\n",
    "            if not model:\n",
    "                model = title\n",
    "            return brand, model\n",
    "    parts = title.split()\n",
    "    return (parts[0], \" \".join(parts[1:])) if parts else (\"\",\"\")\n",
    "\n",
    "def clean_kms(k):\n",
    "    if not k: return \"\"\n",
    "    return re.sub(r'[^\\d\\.]', '', str(k))\n",
    "\n",
    "def find_rupee(text):\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    return m.group(0).strip() if m else \"\"\n",
    "\n",
    "def find_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    return m.group(1).replace(\",\",\"\") if m else \"\"\n",
    "\n",
    "def find_fuel(text):\n",
    "    for f in [\"Petrol\",\"Diesel\",\"CNG\",\"LPG\",\"Electric\",\"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "def find_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# NEW: mileage extraction helper (looks for kmpl / km/kg / km/kWh / etc.)\n",
    "def extract_mileage(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    txt = text.replace(\"\\xa0\",\" \").strip()\n",
    "    # common patterns like \"18.5 kmpl\", \"22 km/kg\", \"120 km/kWh\"\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|km/kg|km/kwh|km/l|kpl|km)', txt, flags=re.I)\n",
    "    if m:\n",
    "        val = m.group(1)\n",
    "        unit = m.group(2)\n",
    "        return f\"{val} {unit}\".strip()\n",
    "    # sometimes \"Mileage: 18.5 kmpl\" or \"18.5kmpl\" or \"ARAI mileage 18 kmpl\"\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l)\\b', txt, flags=re.I)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} {m2.group(2)}\"\n",
    "    # also check \"Mileage - 18.5\" followed by unit nearby\n",
    "    m3 = re.search(r'Mileage[:\\-\\s]*([\\d]{1,3}(?:\\.\\d+)?)', txt, flags=re.I)\n",
    "    if m3:\n",
    "        # try to find unit near the number\n",
    "        after = txt[m3.end(): m3.end()+12]\n",
    "        u = re.search(r'(kmpl|kpl|km/kg|km/kwh|km/l)', after, flags=re.I)\n",
    "        if u:\n",
    "            return f\"{m3.group(1)} {u.group(1)}\"\n",
    "        return m3.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Setup Selenium\n",
    "opts = Options()\n",
    "if HEADLESS:\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "opts.add_argument(\"--no-sandbox\")\n",
    "opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "opts.add_argument(\"--window-size=1920,1080\")\n",
    "opts.add_argument(\"--disable-gpu\")\n",
    "# avoid automation flags where possible (helps some sites)\n",
    "opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "try:\n",
    "    driver.get(START_URL)\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    # detect total listings/pages (best-effort)\n",
    "    soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    text0 = soup0.get_text(\" \", strip=True)\n",
    "    total_listings = None\n",
    "    m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Kolkata', text0, flags=re.I)\n",
    "    if not m:\n",
    "        m = re.search(r'of\\s+([\\d,]+)\\s+results', text0, flags=re.I)\n",
    "    if m:\n",
    "        total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "    per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "    estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "    total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "    if MAX_PAGES_OVERRIDE:\n",
    "        total_pages = MAX_PAGES_OVERRIDE\n",
    "    print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages_to_try={total_pages}\")\n",
    "\n",
    "    # --- collect all listing card containers (and their detail links where present) ---\n",
    "    detail_links = []\n",
    "    cards_collected = []\n",
    "    seen_links = set()\n",
    "    seen_keys = set()  # dedupe by title+price\n",
    "\n",
    "    for p in range(1, total_pages + 1):\n",
    "        page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "        try:\n",
    "            driver.get(page_url)\n",
    "        except Exception:\n",
    "            time.sleep(1.0)\n",
    "            driver.get(page_url)\n",
    "        # aggressively scroll to trigger lazy-load\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_round = 0\n",
    "        while scroll_round < MAX_SCROLL_ROUNDS:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE + random.random()*0.3)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                # small wiggle to force load\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                time.sleep(0.4)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.4)\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "            last_height = new_height\n",
    "            scroll_round += 1\n",
    "\n",
    "        # parse page to find listing *cards*\n",
    "        page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Heuristic: cards often contain a title (h3), price (₹) and kms text.\n",
    "        # We'll find all h3/title nodes and then locate the nearest card container around them.\n",
    "        titles = page_soup.find_all(\"h3\")\n",
    "        for h in titles:\n",
    "            title = h.get_text(\" \", strip=True)\n",
    "            if not title:\n",
    "                continue\n",
    "\n",
    "            # climb up parents to find a card-like container (max 6 levels)\n",
    "            parent = h\n",
    "            container_text = \"\"\n",
    "            detail_href = \"\"\n",
    "            for _ in range(6):\n",
    "                if parent is None:\n",
    "                    break\n",
    "                # gather text\n",
    "                txt = parent.get_text(\" \", strip=True)\n",
    "                if \"₹\" in txt or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', txt, flags=re.I):\n",
    "                    container_text = txt\n",
    "                    # also try to find detail link inside this parent container\n",
    "                    a = parent.find(\"a\", href=True)\n",
    "                    if a:\n",
    "                        href = a[\"href\"]\n",
    "                        abs_href = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                        detail_href = abs_href.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                    break\n",
    "                parent = parent.parent\n",
    "\n",
    "            # fallback: if no container_text found, use h.get_text plus parent.get_text\n",
    "            if not container_text:\n",
    "                parent = h.parent\n",
    "                container_text = parent.get_text(\" \", strip=True) if parent else h.get_text(\" \", strip=True)\n",
    "\n",
    "            # ensure it's likely a listing: should have price or kms or both\n",
    "            if (\"₹\" not in container_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', container_text, flags=re.I) is None):\n",
    "                continue\n",
    "\n",
    "            price = find_rupee(container_text)\n",
    "            kms = find_kms(container_text)\n",
    "            fuel = find_fuel(container_text)\n",
    "            year = find_year(title) or find_year(container_text)\n",
    "            mileage = extract_mileage(container_text)  # <-- extract mileage from card text\n",
    "\n",
    "            brand, model = guess_brand_and_model(title)\n",
    "\n",
    "            # dedupe key\n",
    "            unique_key = (title + \"||\" + (price or \"\")).strip()\n",
    "            if unique_key in seen_keys:\n",
    "                continue\n",
    "            seen_keys.add(unique_key)\n",
    "\n",
    "            if detail_href and detail_href not in seen_links:\n",
    "                seen_links.add(detail_href)\n",
    "            # append raw row (text-based) including mileage\n",
    "            cards_collected.append({\n",
    "                \"Car_name\": title,\n",
    "                \"brand\": brand,\n",
    "                \"model\": model,\n",
    "                \"kms_driven\": kms.replace(\",\",\"\"),\n",
    "                \"fuel_type\": fuel,\n",
    "                \"year_of_manufacture\": year,\n",
    "                \"price\": price,\n",
    "                \"mileage\": mileage,\n",
    "                \"detail_page\": detail_href,\n",
    "                \"page\": p\n",
    "            })\n",
    "\n",
    "        # small pause\n",
    "        time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "        # early stop if we've collected at least detected total_listings\n",
    "        if total_listings and len(seen_keys) >= total_listings:\n",
    "            print(\"Collected detected total_listings, stopping page scan.\")\n",
    "            break\n",
    "\n",
    "        # small progress print every 10 pages\n",
    "        if p % 10 == 0:\n",
    "            print(f\"Scanned page {p}; collected cards so far: {len(cards_collected)}\")\n",
    "\n",
    "    print(f\"Initial collection done: {len(cards_collected)} card rows, detail links discovered: {len(seen_links)}\")\n",
    "\n",
    "    # If detail links exist, visit each detail page to extract more reliable fields (optional but recommended)\n",
    "    # We'll visit only pages that either lack kms/fuel/year/price/mileage to improve data quality.\n",
    "    # This block is slower; set visit_details=False to skip.\n",
    "    visit_details = True\n",
    "    improved_rows = []\n",
    "    visited = 0\n",
    "\n",
    "    if visit_details and len(seen_links) > 0:\n",
    "        for idx, row in enumerate(cards_collected):\n",
    "            # decide whether to open detail page: if any of main fields missing or no detail link present\n",
    "            need_detail = False\n",
    "            if not row[\"kms_driven\"] or not row[\"fuel_type\"] or not row[\"price\"] or not row[\"year_of_manufacture\"] or not row.get(\"mileage\"):\n",
    "                need_detail = True\n",
    "            if row[\"detail_page\"]:\n",
    "                detail_url = row[\"detail_page\"]\n",
    "            else:\n",
    "                detail_url = None\n",
    "            if not need_detail and detail_url:\n",
    "                # keep as is\n",
    "                improved_rows.append(row)\n",
    "                continue\n",
    "\n",
    "            if detail_url:\n",
    "                try:\n",
    "                    # open detail page\n",
    "                    driver.get(detail_url)\n",
    "                    # wait short while\n",
    "                    time.sleep(1.0 + random.random()*0.8)\n",
    "                    dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    page_text = dsoup.get_text(\" \", strip=True)\n",
    "\n",
    "                    # Title from h1/h2 if present\n",
    "                    ttag = dsoup.find([\"h1\",\"h2\"])\n",
    "                    if ttag:\n",
    "                        title_det = ttag.get_text(\" \", strip=True)\n",
    "                        if title_det:\n",
    "                            row[\"Car_name\"] = title_det\n",
    "                            brand, model = guess_brand_and_model(title_det)\n",
    "                            row[\"brand\"] = brand\n",
    "                            row[\"model\"] = model\n",
    "\n",
    "                    # Try to find labeled values first (reliable)\n",
    "                    # Kms (look for label 'Kms Driven' or 'Kms')\n",
    "                    label_kms = dsoup.find(text=re.compile(r'Kms\\s*Driven|Kms|Odometer', flags=re.I))\n",
    "                    if label_kms:\n",
    "                        try:\n",
    "                            val = label_kms.parent.find_next_sibling()\n",
    "                            if val:\n",
    "                                k = re.sub(r'[^\\d]', '', val.get_text(\" \", strip=True))\n",
    "                                if k:\n",
    "                                    row[\"kms_driven\"] = k\n",
    "                        except:\n",
    "                            pass\n",
    "                    # fallback to page text\n",
    "                    if not row[\"kms_driven\"]:\n",
    "                        kf = find_kms(page_text)\n",
    "                        if kf:\n",
    "                            row[\"kms_driven\"] = kf\n",
    "\n",
    "                    # fuel type label\n",
    "                    label_fuel = dsoup.find(text=re.compile(r'Fuel\\s*Type|Fuel', flags=re.I))\n",
    "                    if label_fuel:\n",
    "                        try:\n",
    "                            val = label_fuel.parent.find_next_sibling()\n",
    "                            if val:\n",
    "                                row[\"fuel_type\"] = val.get_text(\" \", strip=True)\n",
    "                        except:\n",
    "                            pass\n",
    "                    if not row[\"fuel_type\"]:\n",
    "                        ff = find_fuel(page_text)\n",
    "                        if ff:\n",
    "                            row[\"fuel_type\"] = ff\n",
    "\n",
    "                    # year\n",
    "                    label_year = dsoup.find(text=re.compile(r'Year\\s*of\\s*Manufacture|Registration\\s*Year|Year', flags=re.I))\n",
    "                    if label_year:\n",
    "                        try:\n",
    "                            val = label_year.parent.find_next_sibling()\n",
    "                            if val:\n",
    "                                yy = find_year(val.get_text(\" \", strip=True))\n",
    "                                if yy:\n",
    "                                    row[\"year_of_manufacture\"] = yy\n",
    "                        except:\n",
    "                            pass\n",
    "                    if not row[\"year_of_manufacture\"]:\n",
    "                        yy = find_year(row[\"Car_name\"]) or find_year(page_text)\n",
    "                        if yy:\n",
    "                            row[\"year_of_manufacture\"] = yy\n",
    "\n",
    "                    # price\n",
    "                    pr = find_rupee(page_text)\n",
    "                    if pr:\n",
    "                        row[\"price\"] = pr\n",
    "\n",
    "                    # MILEAGE extraction on detail page: labeled field or meta-line or page-wide fallback\n",
    "                    mileage_val = \"\"\n",
    "                    # 1) labeled field 'Mileage' or 'Avg. Mileage'\n",
    "                    mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                    if mnode:\n",
    "                        try:\n",
    "                            parent = mnode.parent\n",
    "                            sib = parent.find_next_sibling()\n",
    "                            if sib:\n",
    "                                mileage_val = sib.get_text(\" \", strip=True)\n",
    "                        except:\n",
    "                            mileage_val = \"\"\n",
    "                    # 2) try meta-line near title\n",
    "                    if not mileage_val and ttag:\n",
    "                        nxt = ttag.find_next()\n",
    "                        checks = 0\n",
    "                        while nxt and checks < 8:\n",
    "                            txt = nxt.get_text(\" \", strip=True)\n",
    "                            if txt and ((\"kms\" in txt.lower()) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', txt, flags=re.I)):\n",
    "                                mileage_val = txt\n",
    "                                break\n",
    "                            nxt = nxt.find_next()\n",
    "                            checks += 1\n",
    "                    # 3) fallback to page_text extraction\n",
    "                    if not mileage_val:\n",
    "                        mileage_val = extract_mileage(page_text)\n",
    "\n",
    "                    # normalize mileage\n",
    "                    if mileage_val:\n",
    "                        row[\"mileage\"] = mileage_val\n",
    "\n",
    "                except Exception as e:\n",
    "                    # if detail fetch fails, keep earlier extracted values\n",
    "                    pass\n",
    "\n",
    "                # tiny sleep between detail visits\n",
    "                time.sleep(random.uniform(0.35, 0.9))\n",
    "            improved_rows.append(row)\n",
    "            visited += 1\n",
    "\n",
    "            # checkpoint: save every 200 detail pages processed\n",
    "            if visited % 200 == 0:\n",
    "                df_ck = pd.DataFrame(improved_rows)\n",
    "                df_ck = df_ck.drop_duplicates(subset=[\"detail_page\",\"Car_name\",\"price\"])\n",
    "                df_ck.to_csv(OUTPUT_CSV, index=False)\n",
    "                df_ck.to_excel(OUTPUT_XLSX, index=False)\n",
    "                print(f\"Checkpoint saved after {visited} detail visits: {len(df_ck)} rows.\")\n",
    "\n",
    "    else:\n",
    "        improved_rows = cards_collected\n",
    "\n",
    "    # Final cleaning + dedupe\n",
    "    df = pd.DataFrame(improved_rows)\n",
    "    # normalize strings and numeric kms\n",
    "    for c in [\"Car_name\",\"brand\",\"model\",\"fuel_type\",\"price\",\"detail_page\",\"mileage\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"kms_driven\"] = df[\"kms_driven\"].apply(lambda x: clean_kms(x) if x else \"\")\n",
    "    df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].apply(lambda x: find_year(str(x)) if x else \"\")\n",
    "\n",
    "    # dedupe by detail_page if present else by Car_name+price\n",
    "    if \"detail_page\" in df.columns and df[\"detail_page\"].str.len().sum() > 0:\n",
    "        df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"Car_name\",\"price\"]).reset_index(drop=True)\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    df.to_excel(OUTPUT_XLSX, index=False)\n",
    "    print(f\"Saved cleaned output: {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "    print(\"Total rows collected:\", len(df))\n",
    "    display(df.head(40))\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5f37f-f47f-4756-a556-d25628598aa1",
   "metadata": {},
   "source": [
    "4th stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f69faf8-2ac6-4307-b663-fb9a57c0952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 861 rows from cardekho_used_cars_kolkata_price_fixed.xlsx\n",
      "Rows flagged for repair: 1 (will visit detail pages)\n",
      "[1/1] Failed to load: nan\n",
      "Done. Updated 0 rows. Saved cleaned file to:\n",
      " - cardekho_used_cars_kolkata_price_fixed_cleaned.xlsx\n",
      " - cardekho_used_cars_kolkata_price_fixed_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: Repair mileage & transmission for existing Cardekho file\n",
    "# - Loads /mnt/data/cardekho_used_cars_hyderabad_price_fixed.xlsx\n",
    "# - Visits detail_page for rows missing/invalid mileage or transmission\n",
    "# - Writes back cleaned file (CSV + XLSX)\n",
    "# Requirements: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\n",
    "import re, time, random, os\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_XLSX = \"cardekho_used_cars_kolkata_price_fixed.xlsx\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_kolkata_price_fixed_cleaned.xlsx\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_kolkata_price_fixed_cleaned.csv\"\n",
    "\n",
    "HEADLESS = True                # set False to watch browser\n",
    "DELAY_BETWEEN = (0.6, 1.2)     # polite per-page pause\n",
    "CHECKPOINT_EVERY = 50          # save every N updated rows\n",
    "MAX_RETRIES = 1\n",
    "# --------------------------------\n",
    "\n",
    "if not os.path.exists(INPUT_XLSX):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_XLSX}. Put your file at this path and re-run.\")\n",
    "\n",
    "# --- utility functions ---\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def normalize_mileage(raw):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = str(raw).strip()\n",
    "    s = s.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \").strip()\n",
    "    # try to capture number + unit (common)\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)', s, flags=re.I)\n",
    "    if m:\n",
    "        val = m.group(1)\n",
    "        unit = m.group(2).lower()\n",
    "        # normalize unit names\n",
    "        unit = unit.replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{val} {unit}\"\n",
    "    # tries like \"18.5\" then search for unit nearby\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)', s)\n",
    "    if m2:\n",
    "        # if no unit, just return number\n",
    "        return m2.group(1)\n",
    "    return s\n",
    "\n",
    "def normalize_transmission(raw):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    for t in [\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\",\"Manual\",\"Automatic\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', s, flags=re.I):\n",
    "            # canonicalize\n",
    "            if t.upper() in (\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    # last resort: find words\n",
    "    if re.search(r'\\bmanual\\b', s, flags=re.I):\n",
    "        return \"Manual\"\n",
    "    if re.search(r'\\bautomatic\\b', s, flags=re.I):\n",
    "        return \"Automatic\"\n",
    "    return s.strip()\n",
    "\n",
    "def extract_mileage_from_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # 1) direct patterns e.g. \"18.5 kmpl\"\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        unit = m.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{m.group(1)} {unit}\"\n",
    "    # 2) near 'mileage' keyword: capture window +/- 60 chars\n",
    "    for keyword in [\"mileage\",\"avg. mileage\",\"avg mileage\",\"city mileage\",\"claimed mileage\",\"average mileage\"]:\n",
    "        idx = text.lower().find(keyword)\n",
    "        if idx != -1:\n",
    "            start = max(0, idx-60)\n",
    "            end = min(len(text), idx+80)\n",
    "            ctx = text[start:end]\n",
    "            m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', ctx, flags=re.I)\n",
    "            if m2:\n",
    "                unit = m2.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "                return f\"{m2.group(1)} {unit}\"\n",
    "            # number only\n",
    "            m3 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)', ctx)\n",
    "            if m3:\n",
    "                return m3.group(1)\n",
    "    # 3) any number+unit elsewhere\n",
    "    m4 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', text, flags=re.I)\n",
    "    if m4:\n",
    "        unit = m4.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{m4.group(1)} {unit}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_trans_from_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # look for label/context words\n",
    "    for t in [\"Manual\",\"Automatic\",\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            return normalize_transmission(t)\n",
    "    # also search near keywords 'transmission' or 'gearbox'\n",
    "    idx = text.lower().find(\"transmission\")\n",
    "    if idx == -1:\n",
    "        idx = text.lower().find(\"gearbox\")\n",
    "    if idx != -1:\n",
    "        start = max(0, idx-40)\n",
    "        end = min(len(text), idx+80)\n",
    "        ctx = text[start:end]\n",
    "        for t in [\"Manual\",\"Automatic\",\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "            if re.search(r'\\b' + re.escape(t) + r'\\b', ctx, flags=re.I):\n",
    "                return normalize_transmission(t)\n",
    "        # fallback to any word in ctx\n",
    "        m = re.search(r'\\b(Manual|Automatic|AMT|CVT|DCT|AT|MT)\\b', ctx, flags=re.I)\n",
    "        if m:\n",
    "            return normalize_transmission(m.group(1))\n",
    "    return \"\"\n",
    "\n",
    "# ----------------- load dataset -----------------\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "print(f\"Loaded {len(df)} rows from {INPUT_XLSX}\")\n",
    "\n",
    "# identify rows that need fixing:\n",
    "# Criteria: mileage empty OR transmission empty OR mileage looks like URL/junk (contains 'http' or '/')\n",
    "def mileage_is_bad(val):\n",
    "    if not val or str(val).strip() == \"\":\n",
    "        return True\n",
    "    s = str(val).lower()\n",
    "    if \"http\" in s or \"/\" in s and len(s) > 10:   # simplistic junk heuristics\n",
    "        return True\n",
    "    # if it's non-numeric and non-unit, mark for check\n",
    "    if not re.search(r'\\d', s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def transmission_is_bad(val):\n",
    "    if not val or str(val).strip() == \"\":\n",
    "        return True\n",
    "    s = str(val)\n",
    "    if re.search(r'http|\\/', s):\n",
    "        return True\n",
    "    # ok if contains known token\n",
    "    if re.search(r'\\b(Manual|Automatic|AMT|CVT|DCT|AT|MT)\\b', s, flags=re.I):\n",
    "        return False\n",
    "    return False  # conservative: if present assume ok\n",
    "\n",
    "# build list of indices to fix\n",
    "to_fix_idx = []\n",
    "for i, row in df.iterrows():\n",
    "    mismatch = False\n",
    "    if mileage_is_bad(row.get(\"mileage\", \"\")):\n",
    "        mismatch = True\n",
    "    if transmission_is_bad(row.get(\"transmission\", \"\")):\n",
    "        mismatch = True\n",
    "    # we will only attempt to fix those that have a valid detail_page URL\n",
    "    if mismatch and row.get(\"detail_page\"):\n",
    "        to_fix_idx.append(i)\n",
    "\n",
    "print(f\"Rows flagged for repair: {len(to_fix_idx)} (will visit detail pages)\")\n",
    "\n",
    "if len(to_fix_idx) == 0:\n",
    "    print(\"No rows need fixing. Exiting.\")\n",
    "else:\n",
    "    # Setup Selenium (headful or headless)\n",
    "    opts = Options()\n",
    "    if HEADLESS:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1200,900\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "    updated = 0\n",
    "    try:\n",
    "        for batch_i, idx in enumerate(to_fix_idx, start=1):\n",
    "            row = df.loc[idx]\n",
    "            url = str(row.get(\"detail_page\")).strip()\n",
    "            if not url:\n",
    "                continue\n",
    "            # polite jitter\n",
    "            time.sleep(random.uniform(*DELAY_BETWEEN))\n",
    "            # fetch page\n",
    "            success = False\n",
    "            for attempt in range(MAX_RETRIES+1):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    # let JS run and content load\n",
    "                    time.sleep(1.0 + random.random()*0.8)\n",
    "                    page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    page_text = page_soup.get_text(\" \", strip=True)\n",
    "                    success = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        time.sleep(0.6)\n",
    "                        continue\n",
    "                    else:\n",
    "                        success = False\n",
    "            if not success:\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] Failed to load: {url}\")\n",
    "                continue\n",
    "\n",
    "            # MULTIPLE extraction strategies, most-specific -> fallback\n",
    "            new_mileage = \"\"\n",
    "            new_trans = \"\"\n",
    "\n",
    "            # Strategy A: labeled dt/dd or table tr (common structure)\n",
    "            # dt/dd\n",
    "            try:\n",
    "                dts = page_soup.find_all(\"dt\")\n",
    "                if dts:\n",
    "                    for dt in dts:\n",
    "                        label = text_of(dt).lower()\n",
    "                        if \"mile\" in label:\n",
    "                            dd = dt.find_next_sibling(\"dd\")\n",
    "                            if dd:\n",
    "                                cand = text_of(dd)\n",
    "                                if cand:\n",
    "                                    new_mileage = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                        if \"trans\" in label or \"gear\" in label:\n",
    "                            dd = dt.find_next_sibling(\"dd\")\n",
    "                            if dd:\n",
    "                                new_trans = extract_trans_from_text(text_of(dd)) or normalize_transmission(text_of(dd))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Strategy B: table rows <tr><th>Label</th><td>Value</td>\n",
    "            if not new_mileage or not new_trans:\n",
    "                try:\n",
    "                    for tr in page_soup.find_all(\"tr\"):\n",
    "                        th = tr.find([\"th\",\"td\"])\n",
    "                        tlabel = text_of(th).lower() if th else \"\"\n",
    "                        tvals = [text_of(x) for x in tr.find_all(\"td\")]\n",
    "                        tval = tvals[0] if tvals else \"\"\n",
    "                        if not new_mileage and (\"mileage\" in tlabel or \"avg\" in tlabel and \"mileage\" in tlabel):\n",
    "                            new_mileage = extract_mileage_from_text(tval) or normalize_mileage(tval)\n",
    "                        if not new_trans and (\"transmission\" in tlabel or \"gearbox\" in tlabel or \"gear\" in tlabel):\n",
    "                            new_trans = extract_trans_from_text(tval) or normalize_transmission(tval)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy C: look for elements with class/id containing keywords\n",
    "            if not new_mileage:\n",
    "                try:\n",
    "                    mileage_nodes = page_soup.find_all(attrs={\"class\": re.compile(r\"mile|mileage|avg-mile|avgMileage\", flags=re.I)})\n",
    "                    for n in mileage_nodes:\n",
    "                        cand = text_of(n)\n",
    "                        cand_val = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                        if cand_val:\n",
    "                            new_mileage = cand_val\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not new_trans:\n",
    "                try:\n",
    "                    trans_nodes = page_soup.find_all(attrs={\"class\": re.compile(r\"trans|gear|gearbox\", flags=re.I)})\n",
    "                    for n in trans_nodes:\n",
    "                        cand = text_of(n)\n",
    "                        cand_val = extract_trans_from_text(cand) or normalize_transmission(cand)\n",
    "                        if cand_val:\n",
    "                            new_trans = cand_val\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy D: meta / data- attributes (rare)\n",
    "            if not new_mileage:\n",
    "                try:\n",
    "                    mtag = page_soup.find(\"meta\", attrs={\"name\": re.compile(r\"mileage\", flags=re.I)})\n",
    "                    if mtag and mtag.get(\"content\"):\n",
    "                        cand = mtag.get(\"content\")\n",
    "                        new_mileage = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy E: near-title meta-line (many Cardekho detail pages show compact specs after title)\n",
    "            if not new_mileage or not new_trans:\n",
    "                try:\n",
    "                    htag = page_soup.find([\"h1\",\"h2\"])\n",
    "                    if htag:\n",
    "                        nxt = htag.find_next()\n",
    "                        checks = 0\n",
    "                        while nxt and checks < 10:\n",
    "                            txt = text_of(nxt)\n",
    "                            if txt and ((\"kms\" in txt.lower()) or (\"kmpl\" in txt.lower()) or (\"mileage\" in txt.lower()) or (\"transmission\" in txt.lower()) or (\"gear\" in txt.lower())):\n",
    "                                if not new_mileage:\n",
    "                                    new_mileage = extract_mileage_from_text(txt) or normalize_mileage(txt)\n",
    "                                if not new_trans:\n",
    "                                    new_trans = extract_trans_from_text(txt) or normalize_transmission(txt)\n",
    "                                break\n",
    "                            nxt = nxt.find_next()\n",
    "                            checks += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy F: page-wide regex fallback (last resort)\n",
    "            if not new_mileage:\n",
    "                new_mileage = extract_mileage_from_text(page_text)\n",
    "            if not new_trans:\n",
    "                new_trans = extract_trans_from_text(page_text)\n",
    "\n",
    "            # Final normalization\n",
    "            new_mileage = normalize_mileage(new_mileage)\n",
    "            new_trans = normalize_transmission(new_trans)\n",
    "\n",
    "            # If still empty, try card-level existing values as fallback (do not overwrite good existing)\n",
    "            existing_mileage = df.at[idx, \"mileage\"] if \"mileage\" in df.columns else \"\"\n",
    "            existing_trans = df.at[idx, \"transmission\"] if \"transmission\" in df.columns else \"\"\n",
    "            if not new_mileage and existing_mileage and not mileage_is_bad(existing_mileage):\n",
    "                new_mileage = existing_mileage\n",
    "            if not new_trans and existing_trans and existing_trans.strip():\n",
    "                new_trans = existing_trans\n",
    "\n",
    "            # write back if changed\n",
    "            changed = False\n",
    "            if new_mileage and (str(df.at[idx, \"mileage\"]) != new_mileage):\n",
    "                df.at[idx, \"mileage\"] = new_mileage\n",
    "                changed = True\n",
    "            if new_trans and (str(df.at[idx, \"transmission\"]) != new_trans):\n",
    "                df.at[idx, \"transmission\"] = new_trans\n",
    "                changed = True\n",
    "\n",
    "            if changed:\n",
    "                updated += 1\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] Updated idx={idx}: mileage='{new_mileage}' transmission='{new_trans}'\")\n",
    "            else:\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] No new data for idx={idx}\")\n",
    "\n",
    "            # checkpointing\n",
    "            if updated and updated % CHECKPOINT_EVERY == 0:\n",
    "                df.to_csv(OUTPUT_CSV, index=False)\n",
    "                df.to_excel(OUTPUT_XLSX, index=False)\n",
    "                print(f\"Checkpoint saved after {updated} updates.\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # final save\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    df.to_excel(OUTPUT_XLSX, index=False)\n",
    "    print(f\"Done. Updated {updated} rows. Saved cleaned file to:\\n - {OUTPUT_XLSX}\\n - {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed373b0-bbda-4bc5-aae1-91004e196a6c",
   "metadata": {},
   "source": [
    "5th stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c766a2a-4932-4fde-bd5a-a7ba00ff838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected total_listings=1364, per_page_guess=68, total_pages=21\n",
      "Scanned page 10; rows so far: 878\n",
      "Scanned page 20; rows so far: 882\n",
      "Collected 882 card-level rows, detail links: 860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\1950994378.py:461: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_24828\\1950994378.py:476: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 detail pages...\n",
      "Processed 200 detail pages...\n",
      "Processed 300 detail pages...\n",
      "Processed 400 detail pages...\n",
      "Processed 500 detail pages...\n",
      "Processed 600 detail pages...\n",
      "Processed 700 detail pages...\n",
      "Processed 800 detail pages...\n",
      "Saved 861 rows to cardekho_used_cars_kolkata_price_fixed.csv and cardekho_used_cars_kolkata_price_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cardekho_with_price_fix.py\n",
    "\n",
    "Improved scraper for CarDekho used cars (Hyderabad) with robust price extraction.\n",
    "Only mileage logic improved — rest unchanged.\n",
    "Requires: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------------------------\n",
    "# Path to uploaded file (local). Use this path if you want to read the previously saved file.\n",
    "# Downstream systems can convert this local path into a downloadable URL if needed.\n",
    "UPLOADED_FILE_PATH = \"cardekho_used_cars_pune_kolkata_fixed_cleaned.xlsx\"\n",
    "# ---------------------------\n",
    "\n",
    "# -------- CONFIG ----------\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+kolkata\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_kolkata_price_fixed.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_kolkata_price_fixed.xlsx\"\n",
    "\n",
    "HEADLESS = True\n",
    "MAX_PAGES_OVERRIDE = None\n",
    "MAX_SCROLLS = 40\n",
    "SCROLL_PAUSE = 0.7\n",
    "PAGE_PAUSE = (0.6, 1.4)\n",
    "VISIT_DETAIL_PAGES = True\n",
    "DETAIL_PAUSE = (0.6, 1.2)\n",
    "MAX_DETAIL_RETRIES = 1\n",
    "\n",
    "BRANDS = [\n",
    "    \"Maruti\", \"Hyundai\", \"Tata\", \"Honda\", \"Toyota\", \"Mahindra\", \"Kia\",\n",
    "    \"BMW\", \"Audi\", \"Mercedes-Benz\", \"Mercedes\", \"Renault\", \"MG\", \"Skoda\",\n",
    "    \"Volkswagen\", \"Ford\", \"Nissan\", \"Jeep\", \"Volvo\", \"Land Rover\", \"Jaguar\",\n",
    "    \"Isuzu\", \"Datsun\", \"Chevrolet\", \"Opel\"\n",
    "]\n",
    "# --------------------------\n",
    "\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def guess_brand_and_model(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in name.lower():\n",
    "            model = re.sub(re.escape(b), \"\", name, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+', '', model)\n",
    "            return b, model or name\n",
    "    parts = name.split()\n",
    "    return (parts[0], \" \".join(parts[1:]) if len(parts) > 1 else \"\") if parts else (\"\",\"\")\n",
    "\n",
    "def extract_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_fuel(text):\n",
    "    for f in [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\", \"Electric\", \"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "# ----------------------\n",
    "# IMPROVED MILEAGE LOGIC\n",
    "# ----------------------\n",
    "\n",
    "# convert mpg to kmpl factor\n",
    "_MPG_TO_KMPL = 0.425144\n",
    "\n",
    "def _try_parse_number(s):\n",
    "    \"\"\"Return float or None for first numeric group found.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s2 = str(s).replace(\",\", \"\").replace(\"\\xa0\",\" \").strip()\n",
    "    m = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)', s2)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_mileage(text):\n",
    "    \"\"\"\n",
    "    Robust mileage extraction that:\n",
    "     - prefers explicit units (kmpl, km/l, kpl)\n",
    "     - converts mpg -> kmpl\n",
    "     - ignores pure distance values like '120 km' (treat as invalid)\n",
    "     - returns standardized string like '18.5 kmpl' or '' if not found\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = str(text).replace(\"\\xa0\",\" \").strip()\n",
    "    low = s.lower()\n",
    "\n",
    "    # 1) explicit kmpl / km/l / kpl patterns\n",
    "    m = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(km\\s*/\\s*l|kmpl|kpl|kmperlitre|km per litre)\\b', low, flags=re.I)\n",
    "    if m:\n",
    "        num = float(m.group(1))\n",
    "        # format number: remove .0 if integer else keep up to 2 decimals\n",
    "        num_fmt = int(num) if num.is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 2) mpg -> convert to kmpl\n",
    "    m_mpg = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*mpg\\b', low, flags=re.I)\n",
    "    if m_mpg:\n",
    "        mpg = float(m_mpg.group(1))\n",
    "        kmpl = round(mpg * _MPG_TO_KMPL, 2)\n",
    "        kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "        return f\"{kmpl_fmt} kmpl\"\n",
    "\n",
    "    # 3) patterns like \"18.5 kmpl\" without spaces or with units in mixed-case\n",
    "    m2 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l)\\b', low, flags=re.I)\n",
    "    if m2:\n",
    "        num = float(m2.group(1))\n",
    "        num_fmt = int(num) if num.is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 4) look for 'mileage' keyword and numbers near it\n",
    "    for keyword in (\"mileage\", \"avg. mileage\", \"avg mileage\", \"claimed mileage\", \"claimed fuel economy\"):\n",
    "        idx = low.find(keyword)\n",
    "        if idx != -1:\n",
    "            window = low[max(0, idx-50): idx+80]\n",
    "            m3 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l|mpg)?', window, flags=re.I)\n",
    "            if m3:\n",
    "                num = float(m3.group(1))\n",
    "                unit = m3.group(2)\n",
    "                if unit and \"mpg\" in unit:\n",
    "                    kmpl = round(num * _MPG_TO_KMPL, 2)\n",
    "                    kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "                    return f\"{kmpl_fmt} kmpl\"\n",
    "                # if unit absent, only accept if number plausible for kmpl\n",
    "                if not unit:\n",
    "                    if num <= 50:  # treat as kmpl\n",
    "                        num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "                        return f\"{num_fmt} kmpl\"\n",
    "                else:\n",
    "                    # if unit is kmpl-like handled above; fallback\n",
    "                    num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "                    return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 5) generic number+unit elsewhere on page\n",
    "    m4 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l|mpg)\\b', low, flags=re.I)\n",
    "    if m4:\n",
    "        val = float(m4.group(1))\n",
    "        unit = m4.group(2)\n",
    "        if 'mpg' in unit:\n",
    "            kmpl = round(val * _MPG_TO_KMPL, 2)\n",
    "            kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "            return f\"{kmpl_fmt} kmpl\"\n",
    "        num_fmt = int(val) if val.is_integer() else round(val, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 6) numeric-only fallback: if page has a single small number (<50) assume kmpl\n",
    "    num = _try_parse_number(s)\n",
    "    if num is not None and num <= 50:\n",
    "        num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 7) otherwise likely a distance or invalid — return empty\n",
    "    return \"\"\n",
    "\n",
    "# helper used by the fallback detail extraction (kept unchanged)\n",
    "def extract_mileage_from_text(text):\n",
    "    # reuse improved extract_mileage\n",
    "    return extract_mileage(text)\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "# ----------------------\n",
    "# existing other helpers unchanged\n",
    "# ----------------------\n",
    "\n",
    "def extract_price_from_soup(soup):\n",
    "    # 1. meta tags\n",
    "    meta_selectors = [\n",
    "        ('meta', {'property': 'og:price:amount'}),\n",
    "        ('meta', {'itemprop': 'price'}),\n",
    "        ('meta', {'name': 'price'}),\n",
    "    ]\n",
    "    for tag, attrs in meta_selectors:\n",
    "        mtag = soup.find(tag, attrs=attrs)\n",
    "        if mtag:\n",
    "            val = mtag.get('content') or mtag.get('value') or \"\"\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 2. attributes that often store price\n",
    "    for attr in ('data-price', 'data-offer-price', 'data-srp', 'data-amount', 'data-price-value'):\n",
    "        el = soup.find(attrs={attr: True})\n",
    "        if el:\n",
    "            val = el.get(attr)\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 3. elements with class or id containing 'price' or 'amount'\n",
    "    px = soup.find(lambda tag: tag.name in (\"div\",\"span\",\"p\",\"strong\") and (\n",
    "        tag.get(\"class\") or tag.get(\"id\")\n",
    "    ) and re.search(r'price|amount|selling|srp|finalPrice|carPrice|actual-price', \" \".join((tag.get(\"class\") or []) + [tag.get(\"id\") or \"\"]), flags=re.I))\n",
    "    if px:\n",
    "        txt = text_of(px)\n",
    "        pr = find_rupee_in_text(txt)\n",
    "        if pr:\n",
    "            return pr\n",
    "        if txt:\n",
    "            return txt.strip()\n",
    "\n",
    "    # 4. any visible text near top with rupee sign\n",
    "    top_region = \"\"\n",
    "    head_candidates = soup.find_all([\"header\", \"section\", \"div\"], limit=6)\n",
    "    for c in head_candidates:\n",
    "        t = text_of(c)\n",
    "        if '₹' in t:\n",
    "            top_region = t\n",
    "            break\n",
    "    if not top_region:\n",
    "        top_region = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    pr = find_rupee_in_text(top_region)\n",
    "    if pr:\n",
    "        return pr\n",
    "\n",
    "    # 5. fallback regex on whole page (Lakh/Crore)\n",
    "    p2 = re.search(r'[\\d\\.,]+\\s*(?:Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', soup.get_text(\" \", strip=True))\n",
    "    if p2:\n",
    "        return p2.group(0).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def find_rupee_in_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    m2 = re.search(r'[\\d\\.,]+\\s*(Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    p = find_rupee_in_text(text)\n",
    "    return p\n",
    "\n",
    "def extract_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# ----------------------\n",
    "# main scraping flow (unchanged)\n",
    "# ----------------------\n",
    "\n",
    "def main():\n",
    "    chrome_opts = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "\n",
    "    try:\n",
    "        driver.get(START_URL)\n",
    "        time.sleep(2.0)\n",
    "        soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        text0 = soup0.get_text(\" \", strip=True)\n",
    "\n",
    "        total_listings = None\n",
    "        m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Kolkata', text0, flags=re.I)\n",
    "        if not m:\n",
    "            m = re.search(r'of\\s+([\\d,]+)\\s+results', text0, flags=re.I)\n",
    "        if m:\n",
    "            total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "        per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "        estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "        total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "        if MAX_PAGES_OVERRIDE:\n",
    "            total_pages = MAX_PAGES_OVERRIDE\n",
    "\n",
    "        print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages={total_pages}\")\n",
    "\n",
    "        rows = []\n",
    "        seen_keys = set()\n",
    "        detail_links = []\n",
    "\n",
    "        for p in range(1, total_pages + 1):\n",
    "            page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "            except Exception:\n",
    "                time.sleep(1.0)\n",
    "                driver.get(page_url)\n",
    "\n",
    "            # scroll aggressively\n",
    "            last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            sc = 0\n",
    "            while sc < MAX_SCROLLS:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(SCROLL_PAUSE)\n",
    "                new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_h == last_h:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                    time.sleep(0.4)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0.4)\n",
    "                    new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_h == last_h:\n",
    "                        break\n",
    "                last_h = new_h\n",
    "                sc += 1\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            titles = page_soup.find_all(\"h3\")\n",
    "            for h in titles:\n",
    "                title = text_of(h)\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                container = h\n",
    "                card_text = \"\"\n",
    "                link = \"\"\n",
    "                for _ in range(6):\n",
    "                    if container is None:\n",
    "                        break\n",
    "                    card_text = text_of(container)\n",
    "                    if \"₹\" in card_text or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I):\n",
    "                        a = container.find(\"a\", href=True)\n",
    "                        if a:\n",
    "                            href = a[\"href\"]\n",
    "                            link = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                            link = link.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                        break\n",
    "                    container = container.parent\n",
    "\n",
    "                if not card_text:\n",
    "                    parent = h.parent\n",
    "                    card_text = text_of(parent) if parent else title\n",
    "\n",
    "                if (\"₹\" not in card_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I) is None):\n",
    "                    continue\n",
    "\n",
    "                price_card = extract_price_from_text(card_text)\n",
    "                key = (title + \"||\" + (price_card or \"\")).strip()\n",
    "                if key in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(key)\n",
    "\n",
    "                kms = extract_kms(card_text)\n",
    "                fuel = extract_fuel(card_text)\n",
    "                year = extract_year(title) or extract_year(card_text)\n",
    "                brand = \"\"\n",
    "                model = \"\"\n",
    "                try:\n",
    "                    brand, model = guess_brand_and_model(title)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                rows.append({\n",
    "                    \"Car_name\": title,\n",
    "                    \"brand\": brand,\n",
    "                    \"model\": model,\n",
    "                    \"kms_driven\": kms,\n",
    "                    \"mileage\": \"\",             # will be filled from detail page if available\n",
    "                    \"transmission\": \"\",\n",
    "                    \"fuel_type\": fuel,\n",
    "                    \"year_of_manufacture\": year,\n",
    "                    \"price\": price_card,\n",
    "                    \"detail_page\": link\n",
    "                })\n",
    "\n",
    "                if link:\n",
    "                    detail_links.append(link)\n",
    "\n",
    "            time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "            if total_listings and len(seen_keys) >= total_listings:\n",
    "                print(\"Reached detected total listings; stopping page scan.\")\n",
    "                break\n",
    "\n",
    "            if p % 10 == 0:\n",
    "                print(f\"Scanned page {p}; rows so far: {len(rows)}\")\n",
    "\n",
    "        print(f\"Collected {len(rows)} card-level rows, detail links: {len(set(detail_links))}\")\n",
    "\n",
    "        # Now ensure price/mileage/transmission are filled: visit detail pages for any row missing price/mileage/trans\n",
    "        if VISIT_DETAIL_PAGES and detail_links:\n",
    "            unique_detail_links = []\n",
    "            seen_dl = set()\n",
    "            for u in detail_links:\n",
    "                if u and u not in seen_dl:\n",
    "                    seen_dl.add(u)\n",
    "                    unique_detail_links.append(u)\n",
    "\n",
    "            # map link -> price/mileage/transmission\n",
    "            detail_map = {}\n",
    "\n",
    "            for i, dl in enumerate(unique_detail_links):\n",
    "                # polite pause\n",
    "                if i > 0:\n",
    "                    time.sleep(random.uniform(*DETAIL_PAUSE))\n",
    "                attempt = 0\n",
    "                success = False\n",
    "                while attempt <= MAX_DETAIL_RETRIES and not success:\n",
    "                    try:\n",
    "                        driver.get(dl)\n",
    "                        time.sleep(1.0 + random.random()*0.8)\n",
    "                        dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                        # price from many possible places\n",
    "                        price_val = extract_price_from_soup(dsoup)\n",
    "                        if not price_val:\n",
    "                            price_val = extract_price_from_text(dsoup.get_text(\" \", strip=True))\n",
    "\n",
    "                        # mileage and transmission extraction now uses improved functions\n",
    "                        page_text = dsoup.get_text(\" \", strip=True)\n",
    "                        # try meta-line near title for quick values\n",
    "                        h_tag = dsoup.find([\"h1\",\"h2\"])\n",
    "                        meta_line = \"\"\n",
    "                        if h_tag:\n",
    "                            nxt = h_tag.find_next()\n",
    "                            checks = 0\n",
    "                            while nxt and checks < 8:\n",
    "                                t = text_of(nxt)\n",
    "                                if t and ((\"kms\" in t.lower()) or (\"₹\" in t) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', t, flags=re.I)):\n",
    "                                    meta_line = t\n",
    "                                    break\n",
    "                                nxt = nxt.find_next()\n",
    "                                checks += 1\n",
    "\n",
    "                        # mileage: improved extraction\n",
    "                        mileage_val = \"\"\n",
    "                        mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                        if mnode:\n",
    "                            try:\n",
    "                                parent = mnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    mileage_val = text_of(sib)\n",
    "                            except:\n",
    "                                mileage_val = \"\"\n",
    "                        if not mileage_val:\n",
    "                            # check meta_line then page_text using improved extract_mileage\n",
    "                            mileage_val = extract_mileage(meta_line or page_text) or extract_mileage(page_text)\n",
    "\n",
    "                        # transmission\n",
    "                        trans_val = \"\"\n",
    "                        tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n",
    "                        if tnode:\n",
    "                            try:\n",
    "                                parent = tnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    trans_val = text_of(sib)\n",
    "                            except:\n",
    "                                trans_val = \"\"\n",
    "                        if not trans_val:\n",
    "                            trans_val = extract_transmission(meta_line or page_text)\n",
    "\n",
    "                        # normalize mileage into consistent 'X kmpl' format (handled by extract_mileage)\n",
    "                        mileage_clean = extract_mileage(mileage_val or \"\")\n",
    "                        detail_map[dl] = {\n",
    "                            \"price\": price_val or \"\",\n",
    "                            \"mileage\": mileage_clean or \"\",\n",
    "                            \"transmission\": trans_val or \"\"\n",
    "                        }\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt > MAX_DETAIL_RETRIES:\n",
    "                            detail_map[dl] = {\"price\": \"\", \"mileage\": \"\", \"transmission\": \"\"}\n",
    "                            success = True\n",
    "                        else:\n",
    "                            time.sleep(0.8)\n",
    "\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} detail pages...\")\n",
    "\n",
    "            # merge into rows\n",
    "            for r in rows:\n",
    "                link = r.get(\"detail_page\", \"\")\n",
    "                if link and link in detail_map:\n",
    "                    if not r.get(\"price\"):\n",
    "                        r[\"price\"] = detail_map[link][\"price\"]\n",
    "                    # set mileage/transmission from detail\n",
    "                    if detail_map[link].get(\"mileage\"):\n",
    "                        r[\"mileage\"] = detail_map[link][\"mileage\"]\n",
    "                    if detail_map[link].get(\"transmission\"):\n",
    "                        r[\"transmission\"] = detail_map[link][\"transmission\"]\n",
    "\n",
    "        # final normalization\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            \"Car_name\",\"brand\",\"model\",\"kms_driven\",\"mileage\",\"transmission\",\"fuel_type\",\"year_of_manufacture\",\"price\",\"detail_page\"\n",
    "        ])\n",
    "        df[\"kms_driven\"] = df[\"kms_driven\"].fillna(\"\").astype(str).apply(lambda x: re.sub(r'[^\\d\\.]', '', x))\n",
    "        df[\"mileage\"] = df[\"mileage\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"transmission\"] = df[\"transmission\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "        df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].fillna(\"\").astype(str).apply(lambda x: (re.search(r'\\b(19|20)\\d{2}\\b', x).group(0) if re.search(r'\\b(19|20)\\d{2}\\b', x) else \"\"))\n",
    "        df[\"price\"] = df[\"price\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"fuel_type\"] = df[\"fuel_type\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "\n",
    "        # dedupe\n",
    "        if \"detail_page\" in df.columns and df[\"detail_page\"].str.len().sum() > 0:\n",
    "            df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=[\"Car_name\",\"price\"]).reset_index(drop=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# helpers used in detail extraction (note: improved extract_mileage used above)\n",
    "def extract_mileage_fallback(text):\n",
    "    # kept for compatibility; call improved extractor\n",
    "    return extract_mileage(text)\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ea415-e88a-4595-b922-8496d6e2c6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
