{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea64f58-241c-43f4-816d-a705b88f315e",
   "metadata": {},
   "source": [
    "1st stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a9a58e-55d5-4cfc-b75f-bdf4367eef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected total_listings=1794, per_page_guess=67, total_pages=27\n",
      "Scanned page 10; rows so far: 834\n",
      "Scanned page 20; rows so far: 842\n",
      "Collected 844 card-level rows, detail links: 826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_16936\\4088964073.py:339: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_16936\\4088964073.py:354: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 detail pages...\n",
      "Processed 200 detail pages...\n",
      "Processed 300 detail pages...\n",
      "Processed 400 detail pages...\n",
      "Processed 500 detail pages...\n",
      "Processed 600 detail pages...\n",
      "Processed 700 detail pages...\n",
      "Processed 800 detail pages...\n",
      "Saved 827 rows to cardekho_used_cars_ahmedabad_price_fixed.csv and cardekho_used_cars_ahmedabad_price_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cardekho_with_price_fix.py\n",
    "\n",
    "Improved scraper for CarDekho used cars (Ahmedabad) with robust price extraction.\n",
    "Requires: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------- CONFIG ----------\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+ahmedabad\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_ahmedabad_price_fixed.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_ahmedabad_price_fixed.xlsx\"\n",
    "\n",
    "HEADLESS = True\n",
    "MAX_PAGES_OVERRIDE = None\n",
    "MAX_SCROLLS = 40\n",
    "SCROLL_PAUSE = 0.7\n",
    "PAGE_PAUSE = (0.6, 1.4)\n",
    "VISIT_DETAIL_PAGES = True\n",
    "DETAIL_PAUSE = (0.6, 1.2)\n",
    "MAX_DETAIL_RETRIES = 1\n",
    "\n",
    "BRANDS = [\n",
    "    \"Maruti\", \"Hyundai\", \"Tata\", \"Honda\", \"Toyota\", \"Mahindra\", \"Kia\",\n",
    "    \"BMW\", \"Audi\", \"Mercedes-Benz\", \"Mercedes\", \"Renault\", \"MG\", \"Skoda\",\n",
    "    \"Volkswagen\", \"Ford\", \"Nissan\", \"Jeep\", \"Volvo\", \"Land Rover\", \"Jaguar\",\n",
    "    \"Isuzu\", \"Datsun\", \"Chevrolet\", \"Opel\"\n",
    "]\n",
    "# --------------------------\n",
    "\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def guess_brand_and_model(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in name.lower():\n",
    "            model = re.sub(re.escape(b), \"\", name, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+', '', model)\n",
    "            return b, model or name\n",
    "    parts = name.split()\n",
    "    return (parts[0], \" \".join(parts[1:]) if len(parts) > 1 else \"\") if parts else (\"\",\"\")\n",
    "\n",
    "def extract_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_fuel(text):\n",
    "    for f in [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\", \"Electric\", \"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "# NEW: robust price extraction from soup and raw text\n",
    "def extract_price_from_soup(soup):\n",
    "    # 1. meta tags\n",
    "    meta_selectors = [\n",
    "        ('meta', {'property': 'og:price:amount'}),\n",
    "        ('meta', {'itemprop': 'price'}),\n",
    "        ('meta', {'name': 'price'}),\n",
    "    ]\n",
    "    for tag, attrs in meta_selectors:\n",
    "        mtag = soup.find(tag, attrs=attrs)\n",
    "        if mtag:\n",
    "            val = mtag.get('content') or mtag.get('value') or \"\"\n",
    "            if val:\n",
    "                # normalize: prepend ₹ if numeric and no symbol\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 2. attributes that often store price\n",
    "    for attr in ('data-price', 'data-offer-price', 'data-srp', 'data-amount', 'data-price-value'):\n",
    "        el = soup.find(attrs={attr: True})\n",
    "        if el:\n",
    "            val = el.get(attr)\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 3. elements with class or id containing 'price' or 'amount'\n",
    "    px = soup.find(lambda tag: tag.name in (\"div\",\"span\",\"p\",\"strong\") and (\n",
    "        tag.get(\"class\") or tag.get(\"id\")\n",
    "    ) and re.search(r'price|amount|selling|srp|finalPrice|carPrice|actual-price', \" \".join((tag.get(\"class\") or []) + [tag.get(\"id\") or \"\"]), flags=re.I))\n",
    "    if px:\n",
    "        txt = text_of(px)\n",
    "        pr = find_rupee_in_text(txt)\n",
    "        if pr:\n",
    "            return pr\n",
    "        if txt:\n",
    "            return txt.strip()\n",
    "\n",
    "    # 4. any visible text near top with rupee sign\n",
    "    top_region = \"\"\n",
    "    # try header / top sections\n",
    "    head_candidates = soup.find_all([\"header\", \"section\", \"div\"], limit=6)\n",
    "    for c in head_candidates:\n",
    "        t = text_of(c)\n",
    "        if '₹' in t:\n",
    "            top_region = t\n",
    "            break\n",
    "    if not top_region:\n",
    "        # fallback full page text (takes last resort)\n",
    "        top_region = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    pr = find_rupee_in_text(top_region)\n",
    "    if pr:\n",
    "        return pr\n",
    "\n",
    "    # 5. fallback regex on whole page (Lakh/Crore)\n",
    "    p2 = re.search(r'[\\d\\.,]+\\s*(?:Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', soup.get_text(\" \", strip=True))\n",
    "    if p2:\n",
    "        return p2.group(0).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def find_rupee_in_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    # sometimes rupee symbol is missing but values use lakh/crore\n",
    "    m2 = re.search(r'[\\d\\.,]+\\s*(Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    # ensures same fallback when only raw text available\n",
    "    p = find_rupee_in_text(text)\n",
    "    return p\n",
    "\n",
    "def extract_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# main\n",
    "def main():\n",
    "    chrome_opts = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "\n",
    "    try:\n",
    "        driver.get(START_URL)\n",
    "        time.sleep(2.0)\n",
    "        soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        text0 = soup0.get_text(\" \", strip=True)\n",
    "\n",
    "        total_listings = None\n",
    "        m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Ahmedabad', text0, flags=re.I)\n",
    "        if not m:\n",
    "            m = re.search(r'of\\s+([\\d,]+)\\s+results', text0, flags=re.I)\n",
    "        if m:\n",
    "            total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "        per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "        estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "        total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "        if MAX_PAGES_OVERRIDE:\n",
    "            total_pages = MAX_PAGES_OVERRIDE\n",
    "\n",
    "        print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages={total_pages}\")\n",
    "\n",
    "        rows = []\n",
    "        seen_keys = set()\n",
    "        detail_links = []\n",
    "\n",
    "        for p in range(1, total_pages + 1):\n",
    "            page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "            except Exception:\n",
    "                time.sleep(1.0)\n",
    "                driver.get(page_url)\n",
    "\n",
    "            # scroll aggressively\n",
    "            last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            sc = 0\n",
    "            while sc < MAX_SCROLLS:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(SCROLL_PAUSE)\n",
    "                new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_h == last_h:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                    time.sleep(0.4)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0.4)\n",
    "                    new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_h == last_h:\n",
    "                        break\n",
    "                last_h = new_h\n",
    "                sc += 1\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            titles = page_soup.find_all(\"h3\")\n",
    "            for h in titles:\n",
    "                title = text_of(h)\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                container = h\n",
    "                card_text = \"\"\n",
    "                link = \"\"\n",
    "                for _ in range(6):\n",
    "                    if container is None:\n",
    "                        break\n",
    "                    card_text = text_of(container)\n",
    "                    if \"₹\" in card_text or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I):\n",
    "                        a = container.find(\"a\", href=True)\n",
    "                        if a:\n",
    "                            href = a[\"href\"]\n",
    "                            link = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                            link = link.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                        break\n",
    "                    container = container.parent\n",
    "\n",
    "                if not card_text:\n",
    "                    parent = h.parent\n",
    "                    card_text = text_of(parent) if parent else title\n",
    "\n",
    "                if (\"₹\" not in card_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I) is None):\n",
    "                    continue\n",
    "\n",
    "                price_card = extract_price_from_text(card_text)\n",
    "                key = (title + \"||\" + (price_card or \"\")).strip()\n",
    "                if key in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(key)\n",
    "\n",
    "                kms = extract_kms(card_text)\n",
    "                fuel = extract_fuel(card_text)\n",
    "                year = extract_year(title) or extract_year(card_text)\n",
    "                brand = \"\"\n",
    "                model = \"\"\n",
    "                try:\n",
    "                    brand, model = guess_brand_and_model(title)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                rows.append({\n",
    "                    \"Car_name\": title,\n",
    "                    \"brand\": brand,\n",
    "                    \"model\": model,\n",
    "                    \"kms_driven\": kms,\n",
    "                    \"mileage\": \"\",\n",
    "                    \"transmission\": \"\",\n",
    "                    \"fuel_type\": fuel,\n",
    "                    \"year_of_manufacture\": year,\n",
    "                    \"price\": price_card,\n",
    "                    \"detail_page\": link\n",
    "                })\n",
    "\n",
    "                if link:\n",
    "                    detail_links.append(link)\n",
    "\n",
    "            time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "            if total_listings and len(seen_keys) >= total_listings:\n",
    "                print(\"Reached detected total listings; stopping page scan.\")\n",
    "                break\n",
    "\n",
    "            if p % 10 == 0:\n",
    "                print(f\"Scanned page {p}; rows so far: {len(rows)}\")\n",
    "\n",
    "        print(f\"Collected {len(rows)} card-level rows, detail links: {len(set(detail_links))}\")\n",
    "\n",
    "        # Now ensure price is filled: visit detail pages for any row missing price\n",
    "        if VISIT_DETAIL_PAGES and detail_links:\n",
    "            unique_detail_links = []\n",
    "            seen_dl = set()\n",
    "            for u in detail_links:\n",
    "                if u and u not in seen_dl:\n",
    "                    seen_dl.add(u)\n",
    "                    unique_detail_links.append(u)\n",
    "\n",
    "            # map link -> price (and optionally mileage/transmission)\n",
    "            detail_map = {}\n",
    "\n",
    "            for i, dl in enumerate(unique_detail_links):\n",
    "                # polite pause\n",
    "                if i > 0:\n",
    "                    time.sleep(random.uniform(*DETAIL_PAUSE))\n",
    "                attempt = 0\n",
    "                success = False\n",
    "                while attempt <= MAX_DETAIL_RETRIES and not success:\n",
    "                    try:\n",
    "                        driver.get(dl)\n",
    "                        time.sleep(1.0 + random.random()*0.8)\n",
    "                        dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                        # price from many possible places\n",
    "                        price_val = extract_price_from_soup(dsoup)\n",
    "                        # fallback to regex on page text\n",
    "                        if not price_val:\n",
    "                            price_val = extract_price_from_text(dsoup.get_text(\" \", strip=True))\n",
    "\n",
    "                        # mileage and transmission extraction as before\n",
    "                        page_text = dsoup.get_text(\" \", strip=True)\n",
    "                        # try meta-line near title for quick values\n",
    "                        h_tag = dsoup.find([\"h1\",\"h2\"])\n",
    "                        meta_line = \"\"\n",
    "                        if h_tag:\n",
    "                            nxt = h_tag.find_next()\n",
    "                            checks = 0\n",
    "                            while nxt and checks < 8:\n",
    "                                t = text_of(nxt)\n",
    "                                if t and ((\"kms\" in t.lower()) or (\"₹\" in t) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', t, flags=re.I)):\n",
    "                                    meta_line = t\n",
    "                                    break\n",
    "                                nxt = nxt.find_next()\n",
    "                                checks += 1\n",
    "\n",
    "                        # mileage\n",
    "                        mileage_val = \"\"\n",
    "                        mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                        if mnode:\n",
    "                            try:\n",
    "                                parent = mnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    mileage_val = text_of(sib)\n",
    "                            except:\n",
    "                                mileage_val = \"\"\n",
    "                        if not mileage_val:\n",
    "                            # check meta_line then page_text\n",
    "                            mileage_val = extract_mileage(meta_line or page_text) or extract_mileage(page_text)\n",
    "\n",
    "                        # transmission\n",
    "                        trans_val = \"\"\n",
    "                        tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n",
    "                        if tnode:\n",
    "                            try:\n",
    "                                parent = tnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    trans_val = text_of(sib)\n",
    "                            except:\n",
    "                                trans_val = \"\"\n",
    "                        if not trans_val:\n",
    "                            trans_val = extract_transmission(meta_line or page_text)\n",
    "\n",
    "                        detail_map[dl] = {\n",
    "                            \"price\": price_val or \"\",\n",
    "                            \"mileage\": mileage_val or \"\",\n",
    "                            \"transmission\": trans_val or \"\"\n",
    "                        }\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt > MAX_DETAIL_RETRIES:\n",
    "                            detail_map[dl] = {\"price\": \"\", \"mileage\": \"\", \"transmission\": \"\"}\n",
    "                            success = True\n",
    "                        else:\n",
    "                            time.sleep(0.8)\n",
    "\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} detail pages...\")\n",
    "\n",
    "            # merge into rows\n",
    "            for r in rows:\n",
    "                link = r.get(\"detail_page\", \"\")\n",
    "                if link and link in detail_map:\n",
    "                    # prefer detail price if card-level empty\n",
    "                    if not r.get(\"price\"):\n",
    "                        r[\"price\"] = detail_map[link][\"price\"]\n",
    "                    # always try to fill mileage/trans if available\n",
    "                    if not r.get(\"mileage\"):\n",
    "                        r[\"mileage\"] = detail_map[link][\"mileage\"]\n",
    "                    if not r.get(\"transmission\"):\n",
    "                        r[\"transmission\"] = detail_map[link][\"transmission\"]\n",
    "\n",
    "        # final normalization\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            \"Car_name\",\"brand\",\"model\",\"kms_driven\",\"mileage\",\"transmission\",\"fuel_type\",\"year_of_manufacture\",\"price\",\"detail_page\"\n",
    "        ])\n",
    "        df[\"kms_driven\"] = df[\"kms_driven\"].fillna(\"\").astype(str).apply(lambda x: re.sub(r'[^\\d\\.]', '', x))\n",
    "        df[\"mileage\"] = df[\"mileage\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"transmission\"] = df[\"transmission\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "        df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].fillna(\"\").astype(str).apply(lambda x: (re.search(r'\\b(19|20)\\d{2}\\b', x).group(0) if re.search(r'\\b(19|20)\\d{2}\\b', x) else \"\"))\n",
    "        df[\"price\"] = df[\"price\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"fuel_type\"] = df[\"fuel_type\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "\n",
    "        # dedupe\n",
    "        if \"detail_page\" in df.columns and df[\"detail_page\"].str.len().sum() > 0:\n",
    "            df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=[\"Car_name\",\"price\"]).reset_index(drop=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# helpers used in detail extraction (mileage/transmission)\n",
    "def extract_mileage(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # reuse a robust regex set (common patterns)\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|km/kg|km/kwh|km/l|kpl|km)', text, flags=re.I)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2)}\".strip()\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(mpg)\\b', text, flags=re.I)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} {m2.group(2)}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32140b9-c31a-40d6-89bf-afda18372e2e",
   "metadata": {},
   "source": [
    "2nd stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33ab3cd-2478-4079-a8d4-827519e0cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 827 rows from cardekho_used_cars_ahmedabad_price_fixed.xlsx\n",
      "Rows flagged for repair: 1 (will visit detail pages)\n",
      "[1/1] Failed to load: nan\n",
      "Done. Updated 0 rows. Saved cleaned file to:\n",
      " - cardekho_used_cars_ahmedabad_price_fixed_cleaned.xlsx\n",
      " - cardekho_used_cars_ahmedabad_price_fixed_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: Repair mileage & transmission for existing Cardekho file\n",
    "# - Loads /mnt/data/cardekho_used_cars_ahmedabad_price_fixed.xlsx\n",
    "# - Visits detail_page for rows missing/invalid mileage or transmission\n",
    "# - Writes back cleaned file (CSV + XLSX)\n",
    "# Requirements: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\n",
    "import re, time, random, os\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_XLSX = \"cardekho_used_cars_ahmedabad_price_fixed.xlsx\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_ahmedabad_price_fixed_cleaned.xlsx\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_ahmedabad_price_fixed_cleaned.csv\"\n",
    "\n",
    "HEADLESS = True                # set False to watch browser\n",
    "DELAY_BETWEEN = (0.6, 1.2)     # polite per-page pause\n",
    "CHECKPOINT_EVERY = 50          # save every N updated rows\n",
    "MAX_RETRIES = 1\n",
    "# --------------------------------\n",
    "\n",
    "if not os.path.exists(INPUT_XLSX):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_XLSX}. Put your file at this path and re-run.\")\n",
    "\n",
    "# --- utility functions ---\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def normalize_mileage(raw):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = str(raw).strip()\n",
    "    s = s.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \").strip()\n",
    "    # try to capture number + unit (common)\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)', s, flags=re.I)\n",
    "    if m:\n",
    "        val = m.group(1)\n",
    "        unit = m.group(2).lower()\n",
    "        # normalize unit names\n",
    "        unit = unit.replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{val} {unit}\"\n",
    "    # tries like \"18.5\" then search for unit nearby\n",
    "    m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)', s)\n",
    "    if m2:\n",
    "        # if no unit, just return number\n",
    "        return m2.group(1)\n",
    "    return s\n",
    "\n",
    "def normalize_transmission(raw):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    for t in [\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\",\"Manual\",\"Automatic\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', s, flags=re.I):\n",
    "            # canonicalize\n",
    "            if t.upper() in (\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    # last resort: find words\n",
    "    if re.search(r'\\bmanual\\b', s, flags=re.I):\n",
    "        return \"Manual\"\n",
    "    if re.search(r'\\bautomatic\\b', s, flags=re.I):\n",
    "        return \"Automatic\"\n",
    "    return s.strip()\n",
    "\n",
    "def extract_mileage_from_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # 1) direct patterns e.g. \"18.5 kmpl\"\n",
    "    m = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        unit = m.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{m.group(1)} {unit}\"\n",
    "    # 2) near 'mileage' keyword: capture window +/- 60 chars\n",
    "    for keyword in [\"mileage\",\"avg. mileage\",\"avg mileage\",\"city mileage\",\"claimed mileage\",\"average mileage\"]:\n",
    "        idx = text.lower().find(keyword)\n",
    "        if idx != -1:\n",
    "            start = max(0, idx-60)\n",
    "            end = min(len(text), idx+80)\n",
    "            ctx = text[start:end]\n",
    "            m2 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', ctx, flags=re.I)\n",
    "            if m2:\n",
    "                unit = m2.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "                return f\"{m2.group(1)} {unit}\"\n",
    "            # number only\n",
    "            m3 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)', ctx)\n",
    "            if m3:\n",
    "                return m3.group(1)\n",
    "    # 3) any number+unit elsewhere\n",
    "    m4 = re.search(r'([\\d]{1,3}(?:\\.\\d+)?)\\s*(kmpl|kpl|km/kg|km/kwh|km/l|km|mpg)\\b', text, flags=re.I)\n",
    "    if m4:\n",
    "        unit = m4.group(2).lower().replace(\"kpl\",\"kmpl\").replace(\"km/l\",\"kmpl\")\n",
    "        return f\"{m4.group(1)} {unit}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_trans_from_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # look for label/context words\n",
    "    for t in [\"Manual\",\"Automatic\",\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            return normalize_transmission(t)\n",
    "    # also search near keywords 'transmission' or 'gearbox'\n",
    "    idx = text.lower().find(\"transmission\")\n",
    "    if idx == -1:\n",
    "        idx = text.lower().find(\"gearbox\")\n",
    "    if idx != -1:\n",
    "        start = max(0, idx-40)\n",
    "        end = min(len(text), idx+80)\n",
    "        ctx = text[start:end]\n",
    "        for t in [\"Manual\",\"Automatic\",\"AMT\",\"CVT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "            if re.search(r'\\b' + re.escape(t) + r'\\b', ctx, flags=re.I):\n",
    "                return normalize_transmission(t)\n",
    "        # fallback to any word in ctx\n",
    "        m = re.search(r'\\b(Manual|Automatic|AMT|CVT|DCT|AT|MT)\\b', ctx, flags=re.I)\n",
    "        if m:\n",
    "            return normalize_transmission(m.group(1))\n",
    "    return \"\"\n",
    "\n",
    "# ----------------- load dataset -----------------\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "print(f\"Loaded {len(df)} rows from {INPUT_XLSX}\")\n",
    "\n",
    "# identify rows that need fixing:\n",
    "# Criteria: mileage empty OR transmission empty OR mileage looks like URL/junk (contains 'http' or '/')\n",
    "def mileage_is_bad(val):\n",
    "    if not val or str(val).strip() == \"\":\n",
    "        return True\n",
    "    s = str(val).lower()\n",
    "    if \"http\" in s or \"/\" in s and len(s) > 10:   # simplistic junk heuristics\n",
    "        return True\n",
    "    # if it's non-numeric and non-unit, mark for check\n",
    "    if not re.search(r'\\d', s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def transmission_is_bad(val):\n",
    "    if not val or str(val).strip() == \"\":\n",
    "        return True\n",
    "    s = str(val)\n",
    "    if re.search(r'http|\\/', s):\n",
    "        return True\n",
    "    # ok if contains known token\n",
    "    if re.search(r'\\b(Manual|Automatic|AMT|CVT|DCT|AT|MT)\\b', s, flags=re.I):\n",
    "        return False\n",
    "    return False  # conservative: if present assume ok\n",
    "\n",
    "# build list of indices to fix\n",
    "to_fix_idx = []\n",
    "for i, row in df.iterrows():\n",
    "    mismatch = False\n",
    "    if mileage_is_bad(row.get(\"mileage\", \"\")):\n",
    "        mismatch = True\n",
    "    if transmission_is_bad(row.get(\"transmission\", \"\")):\n",
    "        mismatch = True\n",
    "    # we will only attempt to fix those that have a valid detail_page URL\n",
    "    if mismatch and row.get(\"detail_page\"):\n",
    "        to_fix_idx.append(i)\n",
    "\n",
    "print(f\"Rows flagged for repair: {len(to_fix_idx)} (will visit detail pages)\")\n",
    "\n",
    "if len(to_fix_idx) == 0:\n",
    "    print(\"No rows need fixing. Exiting.\")\n",
    "else:\n",
    "    # Setup Selenium (headful or headless)\n",
    "    opts = Options()\n",
    "    if HEADLESS:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1200,900\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "    updated = 0\n",
    "    try:\n",
    "        for batch_i, idx in enumerate(to_fix_idx, start=1):\n",
    "            row = df.loc[idx]\n",
    "            url = str(row.get(\"detail_page\")).strip()\n",
    "            if not url:\n",
    "                continue\n",
    "            # polite jitter\n",
    "            time.sleep(random.uniform(*DELAY_BETWEEN))\n",
    "            # fetch page\n",
    "            success = False\n",
    "            for attempt in range(MAX_RETRIES+1):\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    # let JS run and content load\n",
    "                    time.sleep(1.0 + random.random()*0.8)\n",
    "                    page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    page_text = page_soup.get_text(\" \", strip=True)\n",
    "                    success = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        time.sleep(0.6)\n",
    "                        continue\n",
    "                    else:\n",
    "                        success = False\n",
    "            if not success:\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] Failed to load: {url}\")\n",
    "                continue\n",
    "\n",
    "            # MULTIPLE extraction strategies, most-specific -> fallback\n",
    "            new_mileage = \"\"\n",
    "            new_trans = \"\"\n",
    "\n",
    "            # Strategy A: labeled dt/dd or table tr (common structure)\n",
    "            # dt/dd\n",
    "            try:\n",
    "                dts = page_soup.find_all(\"dt\")\n",
    "                if dts:\n",
    "                    for dt in dts:\n",
    "                        label = text_of(dt).lower()\n",
    "                        if \"mile\" in label:\n",
    "                            dd = dt.find_next_sibling(\"dd\")\n",
    "                            if dd:\n",
    "                                cand = text_of(dd)\n",
    "                                if cand:\n",
    "                                    new_mileage = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                        if \"trans\" in label or \"gear\" in label:\n",
    "                            dd = dt.find_next_sibling(\"dd\")\n",
    "                            if dd:\n",
    "                                new_trans = extract_trans_from_text(text_of(dd)) or normalize_transmission(text_of(dd))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Strategy B: table rows <tr><th>Label</th><td>Value</td>\n",
    "            if not new_mileage or not new_trans:\n",
    "                try:\n",
    "                    for tr in page_soup.find_all(\"tr\"):\n",
    "                        th = tr.find([\"th\",\"td\"])\n",
    "                        tlabel = text_of(th).lower() if th else \"\"\n",
    "                        tvals = [text_of(x) for x in tr.find_all(\"td\")]\n",
    "                        tval = tvals[0] if tvals else \"\"\n",
    "                        if not new_mileage and (\"mileage\" in tlabel or \"avg\" in tlabel and \"mileage\" in tlabel):\n",
    "                            new_mileage = extract_mileage_from_text(tval) or normalize_mileage(tval)\n",
    "                        if not new_trans and (\"transmission\" in tlabel or \"gearbox\" in tlabel or \"gear\" in tlabel):\n",
    "                            new_trans = extract_trans_from_text(tval) or normalize_transmission(tval)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy C: look for elements with class/id containing keywords\n",
    "            if not new_mileage:\n",
    "                try:\n",
    "                    mileage_nodes = page_soup.find_all(attrs={\"class\": re.compile(r\"mile|mileage|avg-mile|avgMileage\", flags=re.I)})\n",
    "                    for n in mileage_nodes:\n",
    "                        cand = text_of(n)\n",
    "                        cand_val = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                        if cand_val:\n",
    "                            new_mileage = cand_val\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not new_trans:\n",
    "                try:\n",
    "                    trans_nodes = page_soup.find_all(attrs={\"class\": re.compile(r\"trans|gear|gearbox\", flags=re.I)})\n",
    "                    for n in trans_nodes:\n",
    "                        cand = text_of(n)\n",
    "                        cand_val = extract_trans_from_text(cand) or normalize_transmission(cand)\n",
    "                        if cand_val:\n",
    "                            new_trans = cand_val\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy D: meta / data- attributes (rare)\n",
    "            if not new_mileage:\n",
    "                try:\n",
    "                    mtag = page_soup.find(\"meta\", attrs={\"name\": re.compile(r\"mileage\", flags=re.I)})\n",
    "                    if mtag and mtag.get(\"content\"):\n",
    "                        cand = mtag.get(\"content\")\n",
    "                        new_mileage = extract_mileage_from_text(cand) or normalize_mileage(cand)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy E: near-title meta-line (many Cardekho detail pages show compact specs after title)\n",
    "            if not new_mileage or not new_trans:\n",
    "                try:\n",
    "                    htag = page_soup.find([\"h1\",\"h2\"])\n",
    "                    if htag:\n",
    "                        nxt = htag.find_next()\n",
    "                        checks = 0\n",
    "                        while nxt and checks < 10:\n",
    "                            txt = text_of(nxt)\n",
    "                            if txt and ((\"kms\" in txt.lower()) or (\"kmpl\" in txt.lower()) or (\"mileage\" in txt.lower()) or (\"transmission\" in txt.lower()) or (\"gear\" in txt.lower())):\n",
    "                                if not new_mileage:\n",
    "                                    new_mileage = extract_mileage_from_text(txt) or normalize_mileage(txt)\n",
    "                                if not new_trans:\n",
    "                                    new_trans = extract_trans_from_text(txt) or normalize_transmission(txt)\n",
    "                                break\n",
    "                            nxt = nxt.find_next()\n",
    "                            checks += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Strategy F: page-wide regex fallback (last resort)\n",
    "            if not new_mileage:\n",
    "                new_mileage = extract_mileage_from_text(page_text)\n",
    "            if not new_trans:\n",
    "                new_trans = extract_trans_from_text(page_text)\n",
    "\n",
    "            # Final normalization\n",
    "            new_mileage = normalize_mileage(new_mileage)\n",
    "            new_trans = normalize_transmission(new_trans)\n",
    "\n",
    "            # If still empty, try card-level existing values as fallback (do not overwrite good existing)\n",
    "            existing_mileage = df.at[idx, \"mileage\"] if \"mileage\" in df.columns else \"\"\n",
    "            existing_trans = df.at[idx, \"transmission\"] if \"transmission\" in df.columns else \"\"\n",
    "            if not new_mileage and existing_mileage and not mileage_is_bad(existing_mileage):\n",
    "                new_mileage = existing_mileage\n",
    "            if not new_trans and existing_trans and existing_trans.strip():\n",
    "                new_trans = existing_trans\n",
    "\n",
    "            # write back if changed\n",
    "            changed = False\n",
    "            if new_mileage and (str(df.at[idx, \"mileage\"]) != new_mileage):\n",
    "                df.at[idx, \"mileage\"] = new_mileage\n",
    "                changed = True\n",
    "            if new_trans and (str(df.at[idx, \"transmission\"]) != new_trans):\n",
    "                df.at[idx, \"transmission\"] = new_trans\n",
    "                changed = True\n",
    "\n",
    "            if changed:\n",
    "                updated += 1\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] Updated idx={idx}: mileage='{new_mileage}' transmission='{new_trans}'\")\n",
    "            else:\n",
    "                print(f\"[{batch_i}/{len(to_fix_idx)}] No new data for idx={idx}\")\n",
    "\n",
    "            # checkpointing\n",
    "            if updated and updated % CHECKPOINT_EVERY == 0:\n",
    "                df.to_csv(OUTPUT_CSV, index=False)\n",
    "                df.to_excel(OUTPUT_XLSX, index=False)\n",
    "                print(f\"Checkpoint saved after {updated} updates.\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # final save\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    df.to_excel(OUTPUT_XLSX, index=False)\n",
    "    print(f\"Done. Updated {updated} rows. Saved cleaned file to:\\n - {OUTPUT_XLSX}\\n - {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4a89f-0002-463f-9c3e-2140a3cf5d49",
   "metadata": {},
   "source": [
    "3rd stage of scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dd00a3-61fc-4c66-b3e3-f8d5be9b14e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected total_listings=1775, per_page_guess=67, total_pages=27\n",
      "Scanned page 10; rows so far: 764\n",
      "Scanned page 20; rows so far: 764\n",
      "Collected 764 card-level rows, detail links: 746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_16936\\571141922.py:461: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_16936\\571141922.py:476: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 detail pages...\n",
      "Processed 200 detail pages...\n",
      "Processed 300 detail pages...\n",
      "Processed 400 detail pages...\n",
      "Processed 500 detail pages...\n",
      "Processed 600 detail pages...\n",
      "Processed 700 detail pages...\n",
      "Saved 747 rows to cardekho_used_cars_ahmedabad_price_fixed.csv and cardekho_used_cars_ahmedabad_price_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "cardekho_with_price_fix.py\n",
    "\n",
    "Improved scraper for CarDekho used cars (Ahmedabad) with robust price extraction.\n",
    "Only mileage logic improved — rest unchanged.\n",
    "Requires: selenium, webdriver-manager, beautifulsoup4, pandas, openpyxl\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------------------------\n",
    "# Path to uploaded file (local). Use this path if you want to read the previously saved file.\n",
    "# Downstream systems can convert this local path into a downloadable URL if needed.\n",
    "UPLOADED_FILE_PATH = \"cardekho_used_cars_ahmedabad_price_fixed_cleaned.xlsx\"\n",
    "# ---------------------------\n",
    "\n",
    "# -------- CONFIG ----------\n",
    "START_URL = \"https://www.cardekho.com/used-cars+in+ahmedabad\"\n",
    "OUTPUT_CSV = \"cardekho_used_cars_ahmedabad_price_fixed.csv\"\n",
    "OUTPUT_XLSX = \"cardekho_used_cars_ahmedabad_price_fixed.xlsx\"\n",
    "\n",
    "HEADLESS = True\n",
    "MAX_PAGES_OVERRIDE = None\n",
    "MAX_SCROLLS = 40\n",
    "SCROLL_PAUSE = 0.7\n",
    "PAGE_PAUSE = (0.6, 1.4)\n",
    "VISIT_DETAIL_PAGES = True\n",
    "DETAIL_PAUSE = (0.6, 1.2)\n",
    "MAX_DETAIL_RETRIES = 1\n",
    "\n",
    "BRANDS = [\n",
    "    \"Maruti\", \"Hyundai\", \"Tata\", \"Honda\", \"Toyota\", \"Mahindra\", \"Kia\",\n",
    "    \"BMW\", \"Audi\", \"Mercedes-Benz\", \"Mercedes\", \"Renault\", \"MG\", \"Skoda\",\n",
    "    \"Volkswagen\", \"Ford\", \"Nissan\", \"Jeep\", \"Volvo\", \"Land Rover\", \"Jaguar\",\n",
    "    \"Isuzu\", \"Datsun\", \"Chevrolet\", \"Opel\"\n",
    "]\n",
    "# --------------------------\n",
    "\n",
    "def text_of(elem):\n",
    "    return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "def guess_brand_and_model(name):\n",
    "    if not name:\n",
    "        return \"\", \"\"\n",
    "    for b in BRANDS:\n",
    "        if b.lower() in name.lower():\n",
    "            model = re.sub(re.escape(b), \"\", name, flags=re.IGNORECASE).strip()\n",
    "            model = re.sub(r'^[\\-\\:\\–\\—\\s]+', '', model)\n",
    "            return b, model or name\n",
    "    parts = name.split()\n",
    "    return (parts[0], \" \".join(parts[1:]) if len(parts) > 1 else \"\") if parts else (\"\",\"\")\n",
    "\n",
    "def extract_kms(text):\n",
    "    m = re.search(r'([\\d,\\.]+)\\s*(?:kms|km)\\b', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_fuel(text):\n",
    "    for f in [\"Petrol\", \"Diesel\", \"CNG\", \"LPG\", \"Electric\", \"Hybrid\"]:\n",
    "        if re.search(r'\\b' + re.escape(f) + r'\\b', text, flags=re.I):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "# ----------------------\n",
    "# IMPROVED MILEAGE LOGIC\n",
    "# ----------------------\n",
    "\n",
    "# convert mpg to kmpl factor\n",
    "_MPG_TO_KMPL = 0.425144\n",
    "\n",
    "def _try_parse_number(s):\n",
    "    \"\"\"Return float or None for first numeric group found.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s2 = str(s).replace(\",\", \"\").replace(\"\\xa0\",\" \").strip()\n",
    "    m = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)', s2)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_mileage(text):\n",
    "    \"\"\"\n",
    "    Robust mileage extraction that:\n",
    "     - prefers explicit units (kmpl, km/l, kpl)\n",
    "     - converts mpg -> kmpl\n",
    "     - ignores pure distance values like '120 km' (treat as invalid)\n",
    "     - returns standardized string like '18.5 kmpl' or '' if not found\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = str(text).replace(\"\\xa0\",\" \").strip()\n",
    "    low = s.lower()\n",
    "\n",
    "    # 1) explicit kmpl / km/l / kpl patterns\n",
    "    m = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(km\\s*/\\s*l|kmpl|kpl|kmperlitre|km per litre)\\b', low, flags=re.I)\n",
    "    if m:\n",
    "        num = float(m.group(1))\n",
    "        # format number: remove .0 if integer else keep up to 2 decimals\n",
    "        num_fmt = int(num) if num.is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 2) mpg -> convert to kmpl\n",
    "    m_mpg = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*mpg\\b', low, flags=re.I)\n",
    "    if m_mpg:\n",
    "        mpg = float(m_mpg.group(1))\n",
    "        kmpl = round(mpg * _MPG_TO_KMPL, 2)\n",
    "        kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "        return f\"{kmpl_fmt} kmpl\"\n",
    "\n",
    "    # 3) patterns like \"18.5 kmpl\" without spaces or with units in mixed-case\n",
    "    m2 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l)\\b', low, flags=re.I)\n",
    "    if m2:\n",
    "        num = float(m2.group(1))\n",
    "        num_fmt = int(num) if num.is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 4) look for 'mileage' keyword and numbers near it\n",
    "    for keyword in (\"mileage\", \"avg. mileage\", \"avg mileage\", \"claimed mileage\", \"claimed fuel economy\"):\n",
    "        idx = low.find(keyword)\n",
    "        if idx != -1:\n",
    "            window = low[max(0, idx-50): idx+80]\n",
    "            m3 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l|mpg)?', window, flags=re.I)\n",
    "            if m3:\n",
    "                num = float(m3.group(1))\n",
    "                unit = m3.group(2)\n",
    "                if unit and \"mpg\" in unit:\n",
    "                    kmpl = round(num * _MPG_TO_KMPL, 2)\n",
    "                    kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "                    return f\"{kmpl_fmt} kmpl\"\n",
    "                # if unit absent, only accept if number plausible for kmpl\n",
    "                if not unit:\n",
    "                    if num <= 50:  # treat as kmpl\n",
    "                        num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "                        return f\"{num_fmt} kmpl\"\n",
    "                else:\n",
    "                    # if unit is kmpl-like handled above; fallback\n",
    "                    num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "                    return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 5) generic number+unit elsewhere on page\n",
    "    m4 = re.search(r'([0-9]{1,3}(?:\\.[0-9]+)?)\\s*(kmpl|kpl|km/l|mpg)\\b', low, flags=re.I)\n",
    "    if m4:\n",
    "        val = float(m4.group(1))\n",
    "        unit = m4.group(2)\n",
    "        if 'mpg' in unit:\n",
    "            kmpl = round(val * _MPG_TO_KMPL, 2)\n",
    "            kmpl_fmt = int(kmpl) if float(kmpl).is_integer() else kmpl\n",
    "            return f\"{kmpl_fmt} kmpl\"\n",
    "        num_fmt = int(val) if val.is_integer() else round(val, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 6) numeric-only fallback: if page has a single small number (<50) assume kmpl\n",
    "    num = _try_parse_number(s)\n",
    "    if num is not None and num <= 50:\n",
    "        num_fmt = int(num) if float(num).is_integer() else round(num, 2)\n",
    "        return f\"{num_fmt} kmpl\"\n",
    "\n",
    "    # 7) otherwise likely a distance or invalid — return empty\n",
    "    return \"\"\n",
    "\n",
    "# helper used by the fallback detail extraction (kept unchanged)\n",
    "def extract_mileage_from_text(text):\n",
    "    # reuse improved extract_mileage\n",
    "    return extract_mileage(text)\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "# ----------------------\n",
    "# existing other helpers unchanged\n",
    "# ----------------------\n",
    "\n",
    "def extract_price_from_soup(soup):\n",
    "    # 1. meta tags\n",
    "    meta_selectors = [\n",
    "        ('meta', {'property': 'og:price:amount'}),\n",
    "        ('meta', {'itemprop': 'price'}),\n",
    "        ('meta', {'name': 'price'}),\n",
    "    ]\n",
    "    for tag, attrs in meta_selectors:\n",
    "        mtag = soup.find(tag, attrs=attrs)\n",
    "        if mtag:\n",
    "            val = mtag.get('content') or mtag.get('value') or \"\"\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 2. attributes that often store price\n",
    "    for attr in ('data-price', 'data-offer-price', 'data-srp', 'data-amount', 'data-price-value'):\n",
    "        el = soup.find(attrs={attr: True})\n",
    "        if el:\n",
    "            val = el.get(attr)\n",
    "            if val:\n",
    "                if re.search(r'[\\d]', val) and '₹' not in val:\n",
    "                    return \"₹ \" + val.strip()\n",
    "                return val.strip()\n",
    "\n",
    "    # 3. elements with class or id containing 'price' or 'amount'\n",
    "    px = soup.find(lambda tag: tag.name in (\"div\",\"span\",\"p\",\"strong\") and (\n",
    "        tag.get(\"class\") or tag.get(\"id\")\n",
    "    ) and re.search(r'price|amount|selling|srp|finalPrice|carPrice|actual-price', \" \".join((tag.get(\"class\") or []) + [tag.get(\"id\") or \"\"]), flags=re.I))\n",
    "    if px:\n",
    "        txt = text_of(px)\n",
    "        pr = find_rupee_in_text(txt)\n",
    "        if pr:\n",
    "            return pr\n",
    "        if txt:\n",
    "            return txt.strip()\n",
    "\n",
    "    # 4. any visible text near top with rupee sign\n",
    "    top_region = \"\"\n",
    "    head_candidates = soup.find_all([\"header\", \"section\", \"div\"], limit=6)\n",
    "    for c in head_candidates:\n",
    "        t = text_of(c)\n",
    "        if '₹' in t:\n",
    "            top_region = t\n",
    "            break\n",
    "    if not top_region:\n",
    "        top_region = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    pr = find_rupee_in_text(top_region)\n",
    "    if pr:\n",
    "        return pr\n",
    "\n",
    "    # 5. fallback regex on whole page (Lakh/Crore)\n",
    "    p2 = re.search(r'[\\d\\.,]+\\s*(?:Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', soup.get_text(\" \", strip=True))\n",
    "    if p2:\n",
    "        return p2.group(0).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def find_rupee_in_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r'₹\\s*[\\d\\.,\\sA-Za-z]+', text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    m2 = re.search(r'[\\d\\.,]+\\s*(Lakh|lakh|Lakhs|lakhs|Crore|crore|Cr)\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_price_from_text(text):\n",
    "    p = find_rupee_in_text(text)\n",
    "    return p\n",
    "\n",
    "def extract_year(text):\n",
    "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# ----------------------\n",
    "# main scraping flow (unchanged)\n",
    "# ----------------------\n",
    "\n",
    "def main():\n",
    "    chrome_opts = Options()\n",
    "    if HEADLESS:\n",
    "        chrome_opts.add_argument(\"--headless=new\")\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_opts)\n",
    "\n",
    "    try:\n",
    "        driver.get(START_URL)\n",
    "        time.sleep(2.0)\n",
    "        soup0 = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        text0 = soup0.get_text(\" \", strip=True)\n",
    "\n",
    "        total_listings = None\n",
    "        m = re.search(r'([\\d,]{2,})\\s+Second Hand Cars in Ahmedabad', text0, flags=re.I)\n",
    "        if not m:\n",
    "            m = re.search(r'of\\s+([\\d,]+)\\s+results', text0, flags=re.I)\n",
    "        if m:\n",
    "            total_listings = int(m.group(1).replace(\",\", \"\"))\n",
    "\n",
    "        per_page_guess = max(1, len(soup0.find_all(\"h3\")))\n",
    "        estimated_pages = math.ceil(total_listings / per_page_guess) if total_listings else None\n",
    "        total_pages = int(estimated_pages) if estimated_pages else 200\n",
    "        if MAX_PAGES_OVERRIDE:\n",
    "            total_pages = MAX_PAGES_OVERRIDE\n",
    "\n",
    "        print(f\"Detected total_listings={total_listings}, per_page_guess={per_page_guess}, total_pages={total_pages}\")\n",
    "\n",
    "        rows = []\n",
    "        seen_keys = set()\n",
    "        detail_links = []\n",
    "\n",
    "        for p in range(1, total_pages + 1):\n",
    "            page_url = START_URL.rstrip(\"/\") + \"?page=\" + str(p)\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "            except Exception:\n",
    "                time.sleep(1.0)\n",
    "                driver.get(page_url)\n",
    "\n",
    "            # scroll aggressively\n",
    "            last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            sc = 0\n",
    "            while sc < MAX_SCROLLS:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(SCROLL_PAUSE)\n",
    "                new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_h == last_h:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-400);\")\n",
    "                    time.sleep(0.4)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0.4)\n",
    "                    new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_h == last_h:\n",
    "                        break\n",
    "                last_h = new_h\n",
    "                sc += 1\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            titles = page_soup.find_all(\"h3\")\n",
    "            for h in titles:\n",
    "                title = text_of(h)\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                container = h\n",
    "                card_text = \"\"\n",
    "                link = \"\"\n",
    "                for _ in range(6):\n",
    "                    if container is None:\n",
    "                        break\n",
    "                    card_text = text_of(container)\n",
    "                    if \"₹\" in card_text or re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I):\n",
    "                        a = container.find(\"a\", href=True)\n",
    "                        if a:\n",
    "                            href = a[\"href\"]\n",
    "                            link = href if \"cardekho.com\" in href else urljoin(page_url, href)\n",
    "                            link = link.split(\"#\")[0].split(\"?utm\")[0]\n",
    "                        break\n",
    "                    container = container.parent\n",
    "\n",
    "                if not card_text:\n",
    "                    parent = h.parent\n",
    "                    card_text = text_of(parent) if parent else title\n",
    "\n",
    "                if (\"₹\" not in card_text) and (re.search(r'\\b\\d+\\s*(?:kms|km)\\b', card_text, flags=re.I) is None):\n",
    "                    continue\n",
    "\n",
    "                price_card = extract_price_from_text(card_text)\n",
    "                key = (title + \"||\" + (price_card or \"\")).strip()\n",
    "                if key in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(key)\n",
    "\n",
    "                kms = extract_kms(card_text)\n",
    "                fuel = extract_fuel(card_text)\n",
    "                year = extract_year(title) or extract_year(card_text)\n",
    "                brand = \"\"\n",
    "                model = \"\"\n",
    "                try:\n",
    "                    brand, model = guess_brand_and_model(title)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                rows.append({\n",
    "                    \"Car_name\": title,\n",
    "                    \"brand\": brand,\n",
    "                    \"model\": model,\n",
    "                    \"kms_driven\": kms,\n",
    "                    \"mileage\": \"\",             # will be filled from detail page if available\n",
    "                    \"transmission\": \"\",\n",
    "                    \"fuel_type\": fuel,\n",
    "                    \"year_of_manufacture\": year,\n",
    "                    \"price\": price_card,\n",
    "                    \"detail_page\": link\n",
    "                })\n",
    "\n",
    "                if link:\n",
    "                    detail_links.append(link)\n",
    "\n",
    "            time.sleep(random.uniform(*PAGE_PAUSE))\n",
    "\n",
    "            if total_listings and len(seen_keys) >= total_listings:\n",
    "                print(\"Reached detected total listings; stopping page scan.\")\n",
    "                break\n",
    "\n",
    "            if p % 10 == 0:\n",
    "                print(f\"Scanned page {p}; rows so far: {len(rows)}\")\n",
    "\n",
    "        print(f\"Collected {len(rows)} card-level rows, detail links: {len(set(detail_links))}\")\n",
    "\n",
    "        # Now ensure price/mileage/transmission are filled: visit detail pages for any row missing price/mileage/trans\n",
    "        if VISIT_DETAIL_PAGES and detail_links:\n",
    "            unique_detail_links = []\n",
    "            seen_dl = set()\n",
    "            for u in detail_links:\n",
    "                if u and u not in seen_dl:\n",
    "                    seen_dl.add(u)\n",
    "                    unique_detail_links.append(u)\n",
    "\n",
    "            # map link -> price/mileage/transmission\n",
    "            detail_map = {}\n",
    "\n",
    "            for i, dl in enumerate(unique_detail_links):\n",
    "                # polite pause\n",
    "                if i > 0:\n",
    "                    time.sleep(random.uniform(*DETAIL_PAUSE))\n",
    "                attempt = 0\n",
    "                success = False\n",
    "                while attempt <= MAX_DETAIL_RETRIES and not success:\n",
    "                    try:\n",
    "                        driver.get(dl)\n",
    "                        time.sleep(1.0 + random.random()*0.8)\n",
    "                        dsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                        # price from many possible places\n",
    "                        price_val = extract_price_from_soup(dsoup)\n",
    "                        if not price_val:\n",
    "                            price_val = extract_price_from_text(dsoup.get_text(\" \", strip=True))\n",
    "\n",
    "                        # mileage and transmission extraction now uses improved functions\n",
    "                        page_text = dsoup.get_text(\" \", strip=True)\n",
    "                        # try meta-line near title for quick values\n",
    "                        h_tag = dsoup.find([\"h1\",\"h2\"])\n",
    "                        meta_line = \"\"\n",
    "                        if h_tag:\n",
    "                            nxt = h_tag.find_next()\n",
    "                            checks = 0\n",
    "                            while nxt and checks < 8:\n",
    "                                t = text_of(nxt)\n",
    "                                if t and ((\"kms\" in t.lower()) or (\"₹\" in t) or re.search(r'\\b\\d+\\s*(?:kmpl|km/kg|km/kwh|km/l|kpl)\\b', t, flags=re.I)):\n",
    "                                    meta_line = t\n",
    "                                    break\n",
    "                                nxt = nxt.find_next()\n",
    "                                checks += 1\n",
    "\n",
    "                        # mileage: improved extraction\n",
    "                        mileage_val = \"\"\n",
    "                        mnode = dsoup.find(text=re.compile(r'\\bMileage\\b|\\bAvg\\.?\\s*Mileage\\b', flags=re.I))\n",
    "                        if mnode:\n",
    "                            try:\n",
    "                                parent = mnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    mileage_val = text_of(sib)\n",
    "                            except:\n",
    "                                mileage_val = \"\"\n",
    "                        if not mileage_val:\n",
    "                            # check meta_line then page_text using improved extract_mileage\n",
    "                            mileage_val = extract_mileage(meta_line or page_text) or extract_mileage(page_text)\n",
    "\n",
    "                        # transmission\n",
    "                        trans_val = \"\"\n",
    "                        tnode = dsoup.find(text=re.compile(r'\\bTransmission\\b|\\bGearbox\\b', flags=re.I))\n",
    "                        if tnode:\n",
    "                            try:\n",
    "                                parent = tnode.parent\n",
    "                                sib = parent.find_next_sibling()\n",
    "                                if sib:\n",
    "                                    trans_val = text_of(sib)\n",
    "                            except:\n",
    "                                trans_val = \"\"\n",
    "                        if not trans_val:\n",
    "                            trans_val = extract_transmission(meta_line or page_text)\n",
    "\n",
    "                        # normalize mileage into consistent 'X kmpl' format (handled by extract_mileage)\n",
    "                        mileage_clean = extract_mileage(mileage_val or \"\")\n",
    "                        detail_map[dl] = {\n",
    "                            \"price\": price_val or \"\",\n",
    "                            \"mileage\": mileage_clean or \"\",\n",
    "                            \"transmission\": trans_val or \"\"\n",
    "                        }\n",
    "                        success = True\n",
    "                    except Exception as e:\n",
    "                        attempt += 1\n",
    "                        if attempt > MAX_DETAIL_RETRIES:\n",
    "                            detail_map[dl] = {\"price\": \"\", \"mileage\": \"\", \"transmission\": \"\"}\n",
    "                            success = True\n",
    "                        else:\n",
    "                            time.sleep(0.8)\n",
    "\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} detail pages...\")\n",
    "\n",
    "            # merge into rows\n",
    "            for r in rows:\n",
    "                link = r.get(\"detail_page\", \"\")\n",
    "                if link and link in detail_map:\n",
    "                    if not r.get(\"price\"):\n",
    "                        r[\"price\"] = detail_map[link][\"price\"]\n",
    "                    # set mileage/transmission from detail\n",
    "                    if detail_map[link].get(\"mileage\"):\n",
    "                        r[\"mileage\"] = detail_map[link][\"mileage\"]\n",
    "                    if detail_map[link].get(\"transmission\"):\n",
    "                        r[\"transmission\"] = detail_map[link][\"transmission\"]\n",
    "\n",
    "        # final normalization\n",
    "        df = pd.DataFrame(rows, columns=[\n",
    "            \"Car_name\",\"brand\",\"model\",\"kms_driven\",\"mileage\",\"transmission\",\"fuel_type\",\"year_of_manufacture\",\"price\",\"detail_page\"\n",
    "        ])\n",
    "        df[\"kms_driven\"] = df[\"kms_driven\"].fillna(\"\").astype(str).apply(lambda x: re.sub(r'[^\\d\\.]', '', x))\n",
    "        df[\"mileage\"] = df[\"mileage\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"transmission\"] = df[\"transmission\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "        df[\"year_of_manufacture\"] = df[\"year_of_manufacture\"].fillna(\"\").astype(str).apply(lambda x: (re.search(r'\\b(19|20)\\d{2}\\b', x).group(0) if re.search(r'\\b(19|20)\\d{2}\\b', x) else \"\"))\n",
    "        df[\"price\"] = df[\"price\"].fillna(\"\").astype(str).apply(lambda x: x.strip())\n",
    "        df[\"fuel_type\"] = df[\"fuel_type\"].fillna(\"\").astype(str).apply(lambda x: x.strip().title())\n",
    "\n",
    "        # dedupe\n",
    "        if \"detail_page\" in df.columns and df[\"detail_page\"].str.len().sum() > 0:\n",
    "            df = df.drop_duplicates(subset=[\"detail_page\"]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=[\"Car_name\",\"price\"]).reset_index(drop=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {OUTPUT_CSV} and {OUTPUT_XLSX}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# helpers used in detail extraction (note: improved extract_mileage used above)\n",
    "def extract_mileage_fallback(text):\n",
    "    # kept for compatibility; call improved extractor\n",
    "    return extract_mileage(text)\n",
    "\n",
    "def extract_transmission(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    for t in [\"Manual\",\"Automatic\",\"CVT\",\"AMT\",\"DCT\",\"AT\",\"MT\"]:\n",
    "        if re.search(r'\\b' + re.escape(t) + r'\\b', text, flags=re.I):\n",
    "            if t.upper() in (\"AT\",\"AMT\",\"CVT\",\"DCT\",\"MT\"):\n",
    "                return t.upper()\n",
    "            return t.title()\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69423f36-b7e1-4738-b3ad-9929933c6bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
